<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>演好自己的戏，走好自己的路</title>
  
  <subtitle>花生人的苟且人生</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://jungler.com/"/>
  <updated>2020-02-08T11:07:23.964Z</updated>
  <id>http://jungler.com/</id>
  
  <author>
    <name>Dave Chen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Ceph 日志迁移</title>
    <link href="http://jungler.com/2018/12/31/%E6%97%A5%E5%BF%97%E8%BF%81%E7%A7%BB/"/>
    <id>http://jungler.com/2018/12/31/日志迁移/</id>
    <published>2018-12-31T03:08:57.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p>18年最后一天，也是本命年的最后一天了。这一年没有什么大的收获，每天都是浑浑噩噩的过，当初誓要对未来的职业规划做个改变，眼看着又要回到原点了。随着年龄的增加，心态也越来越淡定起来，很多事情即便你愿意去努力，愿意去改变，结果也未必是有什么大的不同。人各有命，也许我的命就是“独善其身”吧。</p><p>过去一段时间一直在看一些benchmark，尤其是关注一些硬件选型对软件性能的影响，这其中就涉及到Ceph journal放在不同类型的盘上对Ceph性能的影响，这里记录的是Ceph在使用filestore的场景下，如何更改journal所在的盘，Ceph版本是”Jewel”。一般的建议是Ceph journal是放在SSD或者NVME上，OSD与journal的比例一般遵循：</p><ul><li>SSD 4-5:1</li><li>NVME 12-18:1</li></ul><p>先来看看为什么要用Journal？</p><blockquote><p>   Speed: The journal enables the Ceph OSD Daemon to commit small writes quickly. Ceph writes small, random i/o to the journal sequentially, which tends to speed up bursty workloads by allowing the backing filesystem more time to coalesce writes. The Ceph OSD Daemon’s journal, however, can lead to spiky performance with short spurts of high-speed writes followed by periods without any write progress as the filesystem catches up to the journal.<br>   Consistency: Ceph OSD Daemons require a filesystem interface that guarantees atomic compound operations. Ceph OSD Daemons write a description of the operation to the journal and apply the operation to the filesystem. This enables atomic updates to an object (for example, placement group metadata). Every few seconds–between filestore max sync interval and filestore min sync interval–the Ceph OSD Daemon stops writes and synchronizes the journal with the filesystem, allowing Ceph OSD Daemons to trim operations from the journal and reuse the space. On failure, Ceph OSD Daemons replay the journal starting after the last synchronization operation.</p></blockquote><p>简言之就是速度和一致性，正因为速度的考量，所以这里最好使用快存储设备，例如SSD； 一致性我一直理解类似为数据库的日志，可以用来做数据恢复。</p><p>默认情况下，Jewel版本的Ceph journal是放在HDD上，且在我系统上看到的是在OSD所在的盘上划分了一个5G大小的分区来用作Journal，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ lsblk</span><br><span class="line">...</span><br><span class="line">sdb      8:16   0   1.8T  0 disk</span><br><span class="line">├─sdb2   8:18   0     5G  0 part</span><br><span class="line">└─sdb1   8:17   0   1.8T  0 part /var/lib/ceph/osd/ceph-0</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>所以我们需要找到这个分区，将其修改为SSD或者NVME的分区，看看Journal放在哪里：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /var/lib/ceph/osd/ceph-0</span></span><br><span class="line"><span class="comment"># ls -l journal</span></span><br><span class="line">lrwxrwxrwx 1 root root 58 12月 26 14:04 journal -&gt; /dev/disk/by-partuuid/389057e5-a099-43b6-952e-ad0bff2e7893</span><br><span class="line"><span class="comment"># ls -l /dev/disk/by-partuuid/389057e5-a099-43b6-952e-ad0bff2e7893</span></span><br><span class="line">lrwxrwxrwx 1 root root 10 11月 13 16:15 /dev/disk/by-partuuid/389057e5-a099-43b6-952e-ad0bff2e7893 -&gt; ../../sdb2</span><br></pre></td></tr></table></figure><p>接下来所要做的不外乎是将其链接到SSD/NVME的一块分区，这里假设sde是一块SSD盘:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ceph节点上有三块OSD，所以从sde（SSD盘符）上划分出三块5G大小的分区，为了方便比较，大小也设置为5G</span></span><br><span class="line"><span class="comment"># parted /dev/sde</span></span><br><span class="line">(parted) mklabel gpt</span><br><span class="line">(parted) mkpart journal-0 1 5G</span><br><span class="line">(parted) mkpart journal-1 5G 10G</span><br><span class="line">(parted) mkpart journal-2 10G 15G</span><br><span class="line"><span class="comment"># 修改owner和group，否则后面可能会有权限问题</span></span><br><span class="line"><span class="comment"># sudo chown ceph:ceph /dev/sde1</span></span><br><span class="line"><span class="comment"># sudo chown ceph:ceph /dev/sde2</span></span><br><span class="line"><span class="comment"># sudo chown ceph:ceph /dev/sde3</span></span><br><span class="line"><span class="comment"># cd /var/lib/ceph/osd/ceph-0</span></span><br><span class="line"><span class="comment"># ceph osd set noout （开启noout以避免rebalance。）</span></span><br><span class="line"><span class="comment"># systemctl stop ceph-osd@0</span></span><br><span class="line"><span class="comment"># ceph-osd -i 0 --flush-journal</span></span><br><span class="line"><span class="comment"># rm /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line"><span class="comment"># 链接journal盘到SSD</span></span><br><span class="line"><span class="comment"># ln -s  /var/lib/ceph/osd/&lt;osd-id&gt;/journal /dev/&lt;ssd-partition-for-journal&gt;</span></span><br><span class="line"><span class="comment"># OSD目录下有一个“journal_uuid”，这个文件需要手动更新，应该是一个bug</span></span><br><span class="line"><span class="comment"># echo $partuuid &gt; journal_uuid</span></span><br><span class="line"><span class="comment"># ceph-osd -i 0 --mkjournal</span></span><br><span class="line"><span class="comment"># systemctl start ceph-osd@0</span></span><br><span class="line"><span class="comment"># ceph osd unset noout</span></span><br></pre></td></tr></table></figure><p>做完上面的步骤，用“ceph-disk list”确认一下是否修改成功，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ceph-disk list</span><br><span class="line">...</span><br><span class="line">/dev/sda :</span><br><span class="line">/dev/sda2 ceph journal</span><br><span class="line">/dev/sda1 ceph data, active, cluster ceph, osd.4, journal /dev/sde3</span><br><span class="line">/dev/sdb :</span><br><span class="line">/dev/sdb2 ceph journal</span><br><span class="line">/dev/sdb1 ceph data, active, cluster ceph, osd.1, journal /dev/sde2</span><br><span class="line">/dev/sdc :</span><br><span class="line">/dev/sdc2 ceph journal</span><br><span class="line">/dev/sdc1 ceph data, active, cluster ceph, osd.0, journal /dev/sde1...<span class="comment"># ls -l /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">lrwxrwxrwx 1 root root 58 11月 13 16:01 /var/lib/ceph/osd/ceph-0/journal -&gt; /dev/disk/by-partuuid/8fa35d73-d973-4ff3-b103-a370a10bf4a1</span><br><span class="line"><span class="comment"># ls -l /dev/disk/by-partuuid/8fa35d73-d973-4ff3-b103-a370a10bf4a1</span></span><br><span class="line">lrwxrwxrwx 1 root root 10 11月 13 16:09 /dev/disk/by-partuuid/8fa35d73-d973-4ff3-b103-a370a10bf4a1 -&gt; ../../sde1</span><br></pre></td></tr></table></figure><p>OSD-0所对应的journal已经修改为SSD盘上的一个分区了，接下来就可以跑一些benchmark做一些对比实验了。</p><p>上面描述的是在一个已有的OSD的基础上做修改，如果是新创建一个OSD，则可以直接journal的位置，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy osd create &#123;node-name&#125;:&#123;disk&#125;[:&#123;path/to/journal&#125;]</span><br><span class="line">ceph-deploy osd create osdserver1:sdb:/dev/ssd1</span><br></pre></td></tr></table></figure><p>跑benchmark过程中发现rados benchmark默认参数下跑出来的结果差别不大，究其原因可以从下面的回复中得到答案。</p><blockquote><p>Hi Dave,<br>The SSD journal will help boost iops &amp; latency which will be more apparent for small block sizes. The rados benchmark default block size is 4M, use the -b option to specify the size. Try at 4k, 32k, 64k …<br>As a side note, this is a rados level test, the rbd image size is not relevant here.</p></blockquote><blockquote><p>Maged.</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;18年最后一天，也是本命年的最后一天了。这一年没有什么大的收获，每天都是浑浑噩噩的过，当初誓要对未来的职业规划做个改变，眼看着又要回到原点了。随着年龄的增加，心态也越来越淡定起来，很多事情即便你愿意去努力，愿意去改变，结果也未必是有什么大的不同。人各有命，也许我的命就是“独
      
    
    </summary>
    
    
      <category term="Ceph" scheme="http://jungler.com/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>OpenStack 归档 - 虚拟机临时存储与块存储</title>
    <link href="http://jungler.com/2018/11/04/OpenStack-%E5%BD%92%E6%A1%A3-%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B8%B4%E6%97%B6%E5%AD%98%E5%82%A8%E4%B8%8E%E5%9D%97%E5%AD%98%E5%82%A8/"/>
    <id>http://jungler.com/2018/11/04/OpenStack-归档-虚拟机临时存储与块存储/</id>
    <published>2018-11-04T07:06:04.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p>总体来说，虚拟机内部的存储分临时存储与可拔插的块存储两部分，所谓临时存储既是指存储空间会随着虚拟机的创建而产生，删除而消亡。而块存储(volume)则可以将用户的数据保存下来，并可以attach到不通的虚机机上。</p><h2 id="默认情况"><a href="#默认情况" class="headerlink" title="默认情况"></a>默认情况</h2><p>默认情况创建一个虚机只有一个盘，mount到root分区，看下下面的例子。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nova boot default --image cirros-0.3.5-x86_64-disk --flavor m1.small --nic net-name=private</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ sudo fdisk -l</span><br><span class="line"></span><br><span class="line">Disk /dev/vda: 21.5 GB, 21474836480 bytes</span><br><span class="line">255 heads, 63 sectors/track, 2610 cylinders, total 41943040 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00000000</span><br><span class="line"></span><br><span class="line">   Device Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/vda1   *       16065    41929649    20956792+  83  Linux</span><br><span class="line">$ df -h</span><br><span class="line">Filesystem                Size      Used Available Use% Mounted on</span><br><span class="line">/dev                    998.3M         0    998.3M   0% /dev</span><br><span class="line">/dev/vda1                23.2M     18.0M      4.0M  82% /</span><br><span class="line">tmpfs                  1001.8M         0   1001.8M   0% /dev/shm</span><br><span class="line">tmpfs                   200.0K     68.0K    132.0K  34% /run</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ lsblk</span><br><span class="line">NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0  20G  0 disk</span><br><span class="line">`-vda1 253:1    0  20G  0 part /</span><br></pre></td></tr></table></figure><h2 id="块设备"><a href="#块设备" class="headerlink" title="块设备"></a>块设备</h2><p>如果希望数据能持久的保存下来，即便虚拟机被删之后，还能找到在之前的数据，可以给虚拟机添加一个块设备。块设备由<em>Cinder</em>服务提供，可以将其理解为一块U盘，可以动态的拔插到的你的电脑上。如下，我们给虚拟机添加一个块设备<em>volume</em>。</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/attach_volume.png" alt=""></p><p>再来看看虚拟机里的存储空间，我们会发现多出来一块盘。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ lsblk</span><br><span class="line">NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0  20G  0 disk</span><br><span class="line">`-vda1 253:1    0  20G  0 part /</span><br><span class="line">vdb    253:16   0   1G  0 disk</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ sudo fdisk -l</span><br><span class="line"></span><br><span class="line">Disk /dev/vda: 21.5 GB, 21474836480 bytes</span><br><span class="line">255 heads, 63 sectors/track, 2610 cylinders, total 41943040 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00000000</span><br><span class="line"></span><br><span class="line">   Device Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/vda1   *       16065    41929649    20956792+  83  Linux</span><br><span class="line"></span><br><span class="line">Disk /dev/vdb: 1073 MB, 1073741824 bytes</span><br><span class="line">16 heads, 63 sectors/track, 2080 cylinders, total 2097152 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00000000</span><br></pre></td></tr></table></figure><h2 id="Ephemeral-与Swap"><a href="#Ephemeral-与Swap" class="headerlink" title="Ephemeral 与Swap"></a>Ephemeral 与Swap</h2><p>另外，可以根据需要在虚拟机里创建一些临时的分区/盘，但这些盘同样会时随着虚拟机的生命周期消亡而消亡。<br>首先创建一个flavor,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ nova flavor-create --ephemeral 20 --swap 512 testeph 7 512 1 1</span><br><span class="line">+----+---------+-----------+------+-----------+------+-------+-------------+-----------+</span><br><span class="line">| ID | Name    | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |</span><br><span class="line">+----+---------+-----------+------+-----------+------+-------+-------------+-----------+</span><br><span class="line">| 7  | testeph | 512       | 1    | 20        | 512  | 1     | 1.0         | True      |</span><br><span class="line">+----+---------+-----------+------+-----------+------+-------+-------------+-----------+</span><br></pre></td></tr></table></figure><p>根据此flavor创建一个虚拟机:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">$ nova boot --image cirros-0.3.5-x86_64-disk --flavor 7 --nic net-name=private --ephemeral size=1 emph1 </span><br><span class="line"></span><br><span class="line">$ sudo fdisk -l</span><br><span class="line"></span><br><span class="line">Disk /dev/vda: 1073 MB, 1073741824 bytes</span><br><span class="line">255 heads, 63 sectors/track, 130 cylinders, total 2097152 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00000000</span><br><span class="line"></span><br><span class="line">Device Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/vda1   *       16065     2088449     1036192+  83  Linux</span><br><span class="line"></span><br><span class="line">Disk /dev/vdb: 1073 MB, 1073741824 bytes</span><br><span class="line">16 heads, 63 sectors/track, 2080 cylinders, total 2097152 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00000000</span><br><span class="line"></span><br><span class="line">Disk /dev/vdb doesn&apos;t contain a valid partition table</span><br><span class="line"></span><br><span class="line">Disk /dev/vdc: 536 MB, 536870912 bytes</span><br><span class="line">16 heads, 63 sectors/track, 1040 cylinders, total 1048576 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00000000</span><br><span class="line"></span><br><span class="line">$ lsblk</span><br><span class="line">NAME   MAJ:MIN RM    SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0      1G  0 disk</span><br><span class="line">`-vda1 253:1    0 1011.9M  0 part /</span><br><span class="line">vdb    253:16   0      1G  0 disk /mnt</span><br><span class="line">vdc    253:32   0    512M  0 disk</span><br></pre></td></tr></table></figure><p>可以看出，swap和ephemeral 是以独立的虚拟磁盘来呈现的。<em>disk.eph0</em> 与 <em>disk.swap</em> 都存放在虚拟机的目录下，这意味着虚拟机删除之后，这些文件也将随之被删除。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls /opt/stack/data/nova/instances/29cc9a74-bebc-429d-a0b8-58fbfe89b2cd</span><br><span class="line">console.log  disk  disk.eph0  disk.info  disk.swap</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;总体来说，虚拟机内部的存储分临时存储与可拔插的块存储两部分，所谓临时存储既是指存储空间会随着虚拟机的创建而产生，删除而消亡。而块存储(volume)则可以将用户的数据保存下来，并可以attach到不通的虚机机上。&lt;/p&gt;
&lt;h2 id=&quot;默认情况&quot;&gt;&lt;a href=&quot;#默认
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>（转）初探Openstack Neutron DVR</title>
    <link href="http://jungler.com/2018/10/21/%E5%88%9D%E6%8E%A2Openstack-Neutron-DVR/"/>
    <id>http://jungler.com/2018/10/21/初探Openstack-Neutron-DVR/</id>
    <published>2018-10-21T07:26:18.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章总结的很好了，偷懒直接转过来，以便日后不时查看。</p><p>首先看一下，没有使用DVR的问题在哪里：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/issues.png" alt=""></p><p>从图中可以明显看到东西向和南北向的流量会集中到网络节点，这会使网络节点成为瓶颈。</p><p>那如果启用的DVR，情况会变成如下：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/dvr.png" alt=""></p><p>对于东西向的流量， 流量会直接在计算节点之间传递。<br>对于南北向的流量，如果有floating ip，流量就直接走计算节点。如果没有floating ip，则会走网络节点。</p><p>我的实验环境如下：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/lab.png" alt=""></p><p>然后起了两个私有网络和一个DVR 路由器，拓扑如下:</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/network.png" alt=""></p><p>注:<br>可以看到每个网络与DVR连接时有两个接口，以private1为例，有10.0.1.1和10.0.1.6。</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/interface1.png" alt=""></p><p>可以看到10.0.1.6是centralized_snat的网关，这个地址是在网络节点上的。<br>10.0.1.1是router_interface_distributed地址，它是在每一个计算节点上的。虚机获取到的默认网关就是这个IP。</p><p>虚机情况如下：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/interface2.png" alt=""></p><p>注:<br>虚机privateX-computeY-VM表示，此虚机起在privateX网络，computeY节点上。在compute1节点的两台虚机拥有floating ip。</p><p>下面分析三种情况下traffic的是怎么走的：</p><ol><li>东西向流量：以private1-compute1-VM和private2-compute2-VM之间的通信为例。</li><li>南北向流量：<br> a) 带floating ip， 以private1-compute1-VM对外通信为例。<br> b) 不带floating ip， 以private1-compute2-VM对外通信为例。</li></ol><p>第一种情况 – 东西向流量<br>首先我们看一下虚机private1-compute1-VM的IP和路由:</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/we1.png" alt=""></p><p>再看一下虚机private2-compute2-VM的IP和路由:</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/we2.png" alt=""></p><p>我们在private1-compute1-VM中ping 10.0.2.5(private2-compute2-VM的IP)。</p><p>当我们ping了之后，在首先会查询private1-compute1-VM的路由表，会将包发送到网关10.0.1.1。那么会首先会发送10.0.1.1的arp请求。<br>arp请求会发送到br-int上。<br>我们可以看到10.0.1.5的port id是4e843b99开头的：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/interface3.png" alt=""></p><p>最终会转发到br-int的qvo4e843b99-fb:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">root@dvr-compute1:~<span class="comment"># ovs-vsctl show</span></span><br><span class="line">67f121bd-cca7-41c2-95ab-23ed85d1305b</span><br><span class="line">    Bridge br-tun</span><br><span class="line">        Port patch-int</span><br><span class="line">            Interface patch-int</span><br><span class="line">                <span class="built_in">type</span>: patch</span><br><span class="line">                options: &#123;peer=patch-tun&#125;</span><br><span class="line">        Port <span class="string">"vxlan-0ae09f91"</span></span><br><span class="line">            Interface <span class="string">"vxlan-0ae09f91"</span></span><br><span class="line">                <span class="built_in">type</span>: vxlan</span><br><span class="line">                options: &#123;df_default=<span class="string">"true"</span>, in_key=flow, local_ip=<span class="string">"10.224.159.141"</span>, out_key=flow, remote_ip=<span class="string">"10.224.159.145"</span>&#125;</span><br><span class="line">        Port <span class="string">"vxlan-0ae09f88"</span></span><br><span class="line">            Interface <span class="string">"vxlan-0ae09f88"</span></span><br><span class="line">                <span class="built_in">type</span>: vxlan</span><br><span class="line">                options: &#123;df_default=<span class="string">"true"</span>, in_key=flow, local_ip=<span class="string">"10.224.159.141"</span>, out_key=flow, remote_ip=<span class="string">"10.224.159.136"</span>&#125;</span><br><span class="line">        Port br-tun</span><br><span class="line">            Interface br-tun</span><br><span class="line">                <span class="built_in">type</span>: internal</span><br><span class="line">    Bridge br-int</span><br><span class="line">        fail_mode: secure</span><br><span class="line">        Port <span class="string">"qvo111517d8-c5"</span></span><br><span class="line">            tag: 2</span><br><span class="line">            Interface <span class="string">"qvo111517d8-c5"</span></span><br><span class="line">        Port patch-tun</span><br><span class="line">            Interface patch-tun</span><br><span class="line">                <span class="built_in">type</span>: patch</span><br><span class="line">                options: &#123;peer=patch-int&#125;</span><br><span class="line">        Port <span class="string">"qr-001d0ed9-01"</span></span><br><span class="line">            tag: 2</span><br><span class="line">            Interface <span class="string">"qr-001d0ed9-01"</span></span><br><span class="line">                <span class="built_in">type</span>: internal</span><br><span class="line">        Port br-int</span><br><span class="line">            Interface br-int</span><br><span class="line">                <span class="built_in">type</span>: internal</span><br><span class="line">        Port <span class="string">"qr-ddbdc784-d7"</span></span><br><span class="line">            tag: 1</span><br><span class="line">            Interface <span class="string">"qr-ddbdc784-d7"</span></span><br><span class="line">                <span class="built_in">type</span>: internal</span><br><span class="line">        Port <span class="string">"qvo4e843b99-fb"</span></span><br><span class="line">            tag: 1</span><br><span class="line">            Interface <span class="string">"qvo4e843b99-fb"</span></span><br><span class="line">    Bridge br-ex</span><br><span class="line">        Port br-ex</span><br><span class="line">            Interface br-ex</span><br><span class="line">                <span class="built_in">type</span>: internal</span><br><span class="line">        Port <span class="string">"fg-081d537b-06"</span></span><br><span class="line">            Interface <span class="string">"fg-081d537b-06"</span></span><br><span class="line">                <span class="built_in">type</span>: internal</span><br><span class="line">    ovs_version: <span class="string">"2.0.2"</span></span><br></pre></td></tr></table></figure><p>而端口qvo4e843b99-fb是属于vlan 1的，arp广播包会转发到”qr-ddbdc784-d7”和”patch-tun”。</p><p>首先看”qr-ddbdc784-d7”，这是interface_distributed的接口：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/interface4.png" alt=""></p><p>这个接口是在compute node的的DVR中的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">root@dvr-compute1:~<span class="comment"># ip netns</span></span><br><span class="line">fip-fbd46644-c70f-4227-a414-862a00cbd1d2</span><br><span class="line">&lt;font color=DarkRed size=5&gt;qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa&lt;/font&gt;</span><br><span class="line">qdhcp-401f678d-4518-446c-9a33-cd2fb054c104</span><br><span class="line">qdhcp-db755841-0764-4a8f-b962-8df008ce6330</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root@dvr-compute1:~<span class="comment"># ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ifconfig</span></span><br><span class="line">lo        Link encap:Local Loopback  </span><br><span class="line">          inet addr:127.0.0.1  Mask:255.0.0.0</span><br><span class="line">          inet6 addr: ::1/128 Scope:Host</span><br><span class="line">          UP LOOPBACK RUNNING  MTU:65536  Metric:1</span><br><span class="line">          RX packets:0 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0 </span><br><span class="line">          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">qr-001d0ed9-01 Link encap:Ethernet  HWaddr fa:16:3e:69:b4:05  </span><br><span class="line">          inet addr:10.0.2.1  Bcast:10.0.2.255  Mask:255.255.255.0</span><br><span class="line">          inet6 addr: fe80::f816:3eff:fe69:b405/64 Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING  MTU:1500  Metric:1</span><br><span class="line">          RX packets:35 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:14 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0 </span><br><span class="line">          RX bytes:3510 (3.5 KB)  TX bytes:1092 (1.0 KB)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;font color=DarkRed size=5&gt;</span><br><span class="line">qr-ddbdc784-d7 Link encap:Ethernet  HWaddr fa:16:3e:66:13:af  </span><br><span class="line">          inet addr:10.0.1.1  Bcast:10.0.1.255  Mask:255.255.255.0</span><br><span class="line">          inet6 addr: fe80::f816:3eff:fe66:13af/64 Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING  MTU:1500  Metric:1</span><br><span class="line">          RX packets:401 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:378 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0 </span><br><span class="line">          RX bytes:38224 (38.2 KB)  TX bytes:36224 (36.2 KB)</span><br><span class="line">&lt;/font&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rfp-0fbb351e<span class="_">-a</span> Link encap:Ethernet  HWaddr ea:5c:56:9a:36:9c  </span><br><span class="line">          inet addr:169.254.31.28  Bcast:0.0.0.0  Mask:255.255.255.254</span><br><span class="line">          inet6 addr: fe80::e85c:56ff:fe9a:369c/64 Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</span><br><span class="line">          RX packets:12 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:12 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:1000 </span><br><span class="line">          RX bytes:1116 (1.1 KB)  TX bytes:1116 (1.1 KB)</span><br></pre></td></tr></table></figure><p>接口qr-ddbdc784-d7拥有10.0.1.1。所以他会响应ARP请求。</p><p>回过头来看”patch-tun”, ARP请求转发到这个接口后，会转发到br-tun。看一下br-tun上的flow, 目前我们只需要看红色部分，他会将目标地址为10.0.1.1的ARP请求丢弃：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@dvr-compute1:~<span class="comment"># ovs-ofctl dump-flows br-tun </span></span><br><span class="line">NXST_FLOW reply (xid=0x4): </span><br><span class="line">。。。</span><br><span class="line">cookie=0x0, duration=64720.432s, table=1, n_packets=4, n_bytes=168, idle_age=64607, priority=3,arp,dl_vlan=1,arp_tpa=10.0.1.1 actions=drop </span><br><span class="line">cookie=0x0, duration=62666.766s, table=1, n_packets=2, n_bytes=84, idle_age=62576, priority=3,arp,dl_vlan=2,arp_tpa=10.0.2.1 actions=drop </span><br><span class="line">。。。</span><br></pre></td></tr></table></figure><p>回到我们虚机，当获取到了10.0.1.1的MAC地址后，会发出如下的包：<br>Dest IP: 10.0.2.5<br>Souce IP: 10.0.1.5<br>Dest MAC: MAC of 10.0.1.1<br>Source MAC: MAC of 10.0.1.5</p><p>之后包被转发到compute1的qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa 的namespace：<br>这里利用了内核的高级路由到了，首先看一下ip rule：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip rule<br>0: from all lookup local<br>32766: from all lookup main<br>32767: from all lookup default<br>32768: from 10.0.1.5 lookup 16<br>32769: from 10.0.2.3 lookup 16<br>167772417: from 10.0.1.1/24 lookup 167772417<br>167772417: from 10.0.1.1/24 lookup 167772417<br>167772673: from 10.0.2.1/24 lookup 167772673 </p><p>可以看到会先查找main表：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip route list table main<br>10.0.1.0/24 dev qr-ddbdc784-d7 proto kernel scope link src 10.0.1.1<br>10.0.2.0/24 dev qr-001d0ed9-01 proto kernel scope link src 10.0.2.1<br>169.254.31.28/31 dev rfp-0fbb351e-a proto kernel scope link src 169.254.31.28</p><p>在main表中满足以下路由:<br>10.0.2.0/24 dev qr-001d0ed9-01 proto kernel scope link src 10.0.2.1<br>因此会从qr-001d0ed9-01转发出去。</p><p>之后需要去查询10.0.2.5的MAC地址， MAC是由neutron使用静态ARP的方式设定的：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip nei<br>10.0.1.5 dev qr-ddbdc784-d7 lladdr fa:16:3e:da:75:6d PERMANENT<br>10.0.2.3 dev qr-001d0ed9-01 lladdr fa:16:3e:a4:fc:98 PERMANENT<br>10.0.1.6 dev qr-ddbdc784-d7 lladdr fa:16:3e:9f:55:67 PERMANENT<br>10.0.2.2 dev qr-001d0ed9-01 lladdr fa:16:3e:13:55:66 PERMANENT</p><p><font color="DarkRed" size="4">10.0.2.5 dev qr-001d0ed9-01 lladdr fa:16:3e:51:99:b8 PERMANENT</font><br>10.0.1.4 dev qr-ddbdc784-d7 lladdr fa:16:3e:da:e3:6e PERMANENT<br>10.0.1.7 dev qr-ddbdc784-d7 lladdr fa:16:3e:14:b8:ec PERMANENT<br>169.254.31.29 dev rfp-0fbb351e-a lladdr 42:0d:9f:49:63:c6 STALE</p><p>由于Neutron知道所有虚机的信息，因此他可以事先设定好静态ARP。<br>至此，我们的ICMP包会变成以下形式从qr-001d0ed9-01转发出去：<br>Dest IP: 10.0.2.5<br>Souce IP: 10.0.1.5<br>Dest MAC: MAC of 10.0.2.5<br>Source MAC: MAC of 10.0.2.1</p><p>当包转发到”br-tun”后，进开始查询openflow表。<br>首先我们看一下br-tun的接口状况：<br>root@dvr-compute1:~# ovs-ofctl show br-tun<br>OFPT_FEATURES_REPLY (xid=0x2): dpid:0000e2b7aa5da34a<br>n_tables:254, n_buffers:256<br>capabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS ARP_MATCH_IP<br>actions: OUTPUT SET_VLAN_VID SET_VLAN_PCP STRIP_VLAN SET_DL_SRC SET_DL_DST SET_NW_SRC SET_NW_DST SET_NW_TOS SET_TP_SRC SET_TP_DST ENQUEUE<br> 1(patch-int): addr:76:ae:9f:b3:bf:c6<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br> 3(vxlan-0ae09f88): addr:92:61:e9:43:dd:99<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br> 4(vxlan-0ae09f91): addr:2e:cc:c0:4a:4e:d4<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br> LOCAL(br-tun): addr:e2:b7:aa:5d:a3:4a<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br>OFPT_GET_CONFIG_REPLY (xid=0x4): frags=normal miss_send_len=0</p><p>首先我们看一下br-tun的flowtable，首先会进入table 0，由于包是从br-int发过来的，因此in_port是patch-int(1)，之后会查询表1：<br> cookie=0x0, duration=66172.51s, table=0, n_packets=58, n_bytes=5731, idle_age=20810, hard_age=65534, priority=1,in_port=3 actions=resubmit(,4)<br> cookie=0x0, duration=67599.526s, table=0, n_packets=273, n_bytes=24999, idle_age=1741, hard_age=65534, priority=1,in_port=1 actions=resubmit(,1)<br> cookie=0x0, duration=64437.052s, table=0, n_packets=28, n_bytes=2980, idle_age=20799, priority=1,in_port=4 actions=resubmit(,4)<br> cookie=0x0, duration=67601.704s, table=0, n_packets=5, n_bytes=390, idle_age=65534, hard_age=65534, priority=0 actions=drop</p><p>表1，这张表中会丢弃目标地址是interface_distributed接口的ARP和目的MAC是interface_distributed的包。以防止虚机发送给本地IR的包不会被转发到网络中。<br>我们的ICMP包会命中一下flow，它会把源MAC地址改为全局唯一和计算节点绑定的MAC:<br> cookie=0x0, duration=66135.811s, table=1, n_packets=140, n_bytes=13720, idle_age=65534, hard_age=65534, priority=1,dl_vlan=1,dl_src=fa:16:3e:66:13:af actions=mod_dl_src:fa:16:3f:fe:49:e9,resubmit(,2)<br> cookie=0x0, duration=64082.141s, table=1, n_packets=2, n_bytes=200, idle_age=64081, priority=1,dl_vlan=2,dl_src=fa:16:3e:69:b4:05 actions=mod_dl_src:fa:16:3f:fe:49:e9,resubmit(,2)<br> cookie=0x0, duration=66135.962s, table=1, n_packets=1, n_bytes=98, idle_age=65301, hard_age=65534, priority=2,dl_vlan=1,dl_dst=fa:16:3e:66:13:af actions=drop<br> cookie=0x0, duration=64082.297s, table=1, n_packets=0, n_bytes=0, idle_age=64082, priority=2,dl_vlan=2,dl_dst=fa:16:3e:69:b4:05 actions=drop<br> cookie=0x0, duration=66136.115s, table=1, n_packets=4, n_bytes=168, idle_age=65534, hard_age=65534, priority=3,arp,dl_vlan=1,arp_tpa=10.0.1.1 actions=drop<br> cookie=0x0, duration=64082.449s, table=1, n_packets=2, n_bytes=84, idle_age=63991, priority=3,arp,dl_vlan=2,arp_tpa=10.0.2.1 actions=drop<br> cookie=0x0, duration=67599.22s, table=1, n_packets=123, n_bytes=10687, idle_age=1741, hard_age=65534, priority=0 actions=resubmit(,2)</p><p>这个全局唯一和计算节点绑定的MAC地址，是由neutron全局分配的，数据库中可以看到这个MAC是每个host一个：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/mac1.png" alt=""></p><p>它的base MAC是可以在neutron.conf中配置的：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/mac2.png" alt=""></p><p>继续查询流表2，表2是VXLAN表，如果是广播包就会查询表22，如果是单播包就查询表20：<br> cookie=0x0, duration=67601.554s, table=2, n_packets=176, n_bytes=16981, idle_age=20810, hard_age=65534, priority=0,dl_dst=00:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,20)<br> cookie=0x0, duration=67601.406s, table=2, n_packets=92, n_bytes=7876, idle_age=1741, hard_age=65534, priority=0,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,22)</p><p>ICMP包是单播包，因此会查询表20，由于开启了L2 pop功能，在表20中会事先学习到应该转发到哪个VTEP。<br> cookie=0x0, duration=64076.431s, table=20, n_packets=0, n_bytes=0, idle_age=64076, priority=2,dl_vlan=2,dl_dst=fa:16:3e:13:55:66 actions=strip_vlan,set_tunnel:0x3eb,output:3<br> cookie=0x0, duration=66130.899s, table=20, n_packets=152, n_bytes=14728, idle_age=65534, hard_age=65534, priority=2,dl_vlan=1,dl_dst=fa:16:3e:9f:55:67 actions=strip_vlan,set_tunnel:0x3e9,output:3<br> cookie=0x0, duration=66560.59s, table=20, n_packets=7, n_bytes=552, idle_age=65534, hard_age=65534, priority=2,dl_vlan=1,dl_dst=fa:16:3e:da:e3:6e actions=strip_vlan,set_tunnel:0x3e9,output:2<br> cookie=0x0, duration=64436.717s, table=20, n_packets=0, n_bytes=0, idle_age=64436, priority=2,dl_vlan=1,dl_dst=fa:16:3e:14:b8:ec actions=strip_vlan,set_tunnel:0x3e9,output:4<br> cookie=0x0, duration=64015.308s, table=20, n_packets=0, n_bytes=0, idle_age=64015, priority=2,dl_vlan=2,dl_dst=fa:16:3e:51:99:b8 actions=strip_vlan,set_tunnel:0x3eb,output:4<br> cookie=0x0, duration=64032.699s, table=20, n_packets=9, n_bytes=917, idle_age=20810, priority=2,dl_vlan=2,dl_dst=fa:16:3e:bb:cf:66 actions=strip_vlan,set_tunnel:0x3eb,output:3<br> cookie=0x0, duration=67600.802s, table=20, n_packets=8, n_bytes=784, idle_age=65534, hard_age=65534, priority=0 actions=resubmit(,22)</p><p>注：<br>由于L2 POP并不是本文的重点。因此不在此细说。如果有兴趣可以看以下blog:<br><a href="http://assafmuller.com/category/overlays/" target="_blank" rel="noopener">http://assafmuller.com/category/overlays/</a></p><p>此时包会变成如下形式：<br>Dest IP: 10.0.2.5<br>Souce IP: 10.0.1.5<br>Dest MAC: MAC of 10.0.2.5<br>Source MAC: fa:16:3f:fe:49:e9</p><p>之后包会从port 4发出：<br>root@dvr-compute1:~# ovs-vsctl show<br>67f121bd-cca7-41c2-95ab-23ed85d1305b<br>    Bridge br-tun<br>        Port patch-int<br>            Interface patch-int<br>                type: patch<br>                options: {peer=patch-tun}<br>        Port “vxlan-0ae09f91”<br>            Interface “vxlan-0ae09f91”<br>                type: vxlan<br>                options: {df_default=”true”, in_key=flow, local_ip=”10.224.159.141”, out_key=flow, remote_ip=”10.224.159.145”}<br>        Port “vxlan-0ae09f88”<br>            Interface “vxlan-0ae09f88”<br>                type: vxlan<br>                options: {df_default=”true”, in_key=flow, local_ip=”10.224.159.141”, out_key=flow, remote_ip=”10.224.159.136”}<br>        Port br-tun<br>            Interface br-tun<br>                type: internal</p><p>OVS会将此包进行VXLAN封装，将L2帧分装到VXLAN中，包头如下：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/vlan.png" alt=""></p><p>OVS会将此包进行VXLAN封装，将L2帧分装到VXLAN中，包头如下：<br>本文并不想具体讨论VXLAN是如何封装的，简单的说就是讲二层帧封到了一个UDP包中。</p><p>之后compute2会收到这个包，在compute2的br-tun上查询流表:<br>首先看一下接口情况：<br>root@dvr-compute2:~# ovs-ofctl show br-tun<br>OFPT_FEATURES_REPLY (xid=0x2): dpid:000062e9fb8b8f42<br>n_tables:254, n_buffers:256<br>capabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS ARP_MATCH_IP<br>actions: OUTPUT SET_VLAN_VID SET_VLAN_PCP STRIP_VLAN SET_DL_SRC SET_DL_DST SET_NW_SRC SET_NW_DST SET_NW_TOS SET_TP_SRC SET_TP_DST ENQUEUE<br> 1(patch-int): addr:02:dc:f1:96:db:bd<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br> 3(vxlan-0ae09f88): addr:b6:4b:d0:83:07:52<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br> 4(vxlan-0ae09f8d): addr:12:e5:36:2c:1a:36<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br> LOCAL(br-tun): addr:62:e9:fb:8b:8f:42<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br>OFPT_GET_CONFIG_REPLY (xid=0x4): frags=normal miss_send_len=0</p><p>在table0中可以看到，如果包是从外部发来的就会去查询表4：<br> cookie=0x0, duration=66293.658s, table=0, n_packets=31, n_bytes=3936, idle_age=22651, hard_age=65534, priority=1,in_port=3 actions=resubmit(,4)<br> cookie=0x0, duration=69453.368s, table=0, n_packets=103, n_bytes=9360, idle_age=22651, hard_age=65534, priority=1,in_port=1 actions=resubmit(,1)<br> cookie=0x0, duration=66292.808s, table=0, n_packets=20, n_bytes=1742, idle_age=3598, hard_age=65534, priority=1,in_port=4 actions=resubmit(,4)<br> cookie=0x0, duration=69455.675s, table=0, n_packets=5, n_bytes=390, idle_age=65534, hard_age=65534, priority=0 actions=drop</p><p>在表4中，会将tun_id对应的改为本地vlan id，之后查询表9:<br> cookie=0x0, duration=65937.871s, table=4, n_packets=32, n_bytes=3653, idle_age=22651, hard_age=65534, priority=1,tun_id=0x3eb actions=mod_vlan_vid:3,resubmit(,9)<br> cookie=0x0, duration=66294.732s, table=4, n_packets=19, n_bytes=2025, idle_age=3598, hard_age=65534, priority=1,tun_id=0x3e9 actions=mod_vlan_vid:2,resubmit(,9)<br> cookie=0x0, duration=69455.115s, table=4, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=0 actions=drop</p><p>在表9中，如果发现包的源地址是全局唯一并与计算节点绑定的MAC地址，就将其转发到br-int:<br> cookie=0x0, duration=69453.507s, table=9, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=1,dl_src=fa:16:3f:fe:49:e9 actions=output:1<br> cookie=0x0, duration=69453.782s, table=9, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=1,dl_src=fa:16:3f:72:3f:a7 actions=output:1<br> cookie=0x0, duration=69453.23s, table=9, n_packets=56, n_bytes=6028, idle_age=3598, hard_age=65534, priority=0 actions=resubmit(,10)</p><p>由于我们的源MAC为fa:16:3f:fe:49:e9，我们的ICMP包就被转发到了br-int，之后查询br-int的流表：<br>在表0中，如果是全局唯一并与计算节点绑定的MAC地址就查询表1，否则就正常转发：<br> cookie=0x0, duration=70039.903s, table=0, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=2,in_port=6,dl_src=fa:16:3f:72:3f:a7 actions=resubmit(,1)<br> cookie=0x0, duration=70039.627s, table=0, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=2,in_port=6,dl_src=fa:16:3f:fe:49:e9 actions=resubmit(,1)<br> cookie=0x0, duration=70040.053s, table=0, n_packets=166, n_bytes=15954, idle_age=4184, hard_age=65534, priority=1 actions=NORMAL</p><p>在表1中，事先设定好了flow，如果目的MAC是发送给private2-compute2-VM，就将源MAC改为private2的网关MAC地址：<br> cookie=0x0, duration=66458.695s, table=1, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=4,dl_vlan=3,dl_dst=fa:16:3e:51:99:b8 actions=strip_vlan,mod_dl_src:fa:16:3e:69:b4:05,output:12<br> cookie=0x0, duration=66877.515s, table=1, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=4,dl_vlan=2,dl_dst=fa:16:3e:14:b8:ec actions=strip_vlan,mod_dl_src:fa:16:3e:66:13:af,output:9<br> cookie=0x0, duration=66877.369s, table=1, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=2,ip,dl_vlan=2,nw_dst=10.0.1.0/24 actions=strip_vlan,mod_dl_src:fa:16:3e:66:13:af,output:9<br> cookie=0x0, duration=66458.559s, table=1, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=2,ip,dl_vlan=3,nw_dst=10.0.2.0/24 actions=strip_vlan,mod_dl_src:fa:16:3e:69:b4:05,output:12</p><p>还可以看到下面两条rule是网段flow的rule，他们的output是一个list，会将此包转发到所有连接到此network上。<br>如果所有的虚机的rule都已经事先设定好的话，这两条rule应该并没有实际作用，等到代码稳定后，这两条rule应该会被删除。</p><p>经过br-int的流表后，包会变成如下形式：<br>Dest IP: 10.0.2.5<br>Souce IP: 10.0.1.5<br>Dest MAC: MAC of 10.0.2.5<br>Source MAC: fa:16:3e:69:b4:05(MAC of 10.0.2.1 网关地址)</p><p>至此，虚机private2-compute2-VM就会收到来自private1-compute1-VM的包了。从通信的过程可以看到，跨网段的东西向流量没有经过网络节点。</p><p>第二种情况 – 南北向流量(虚机有floating ip)<br>以虚机private1-compute1-VM对外通信为例，此虚机拥有floating ip:</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/interface-last.png" alt=""></p><p>比如我们在虚机中ping 8.8.8.8 。首先在虚机中查询路由，和第一种情况一样，虚机会发送给网关。发送的包如下：<br>Dest IP: 8.8.8.8<br>Souce IP: 10.0.1.5<br>Dest MAC: MAC of 10.0.1.1<br>Source MAC: MAC of 10.0.1.5</p><p>查看ip rule:<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip rule<br>0: from all lookup local<br>32766: from all lookup main<br>32767: from all lookup default<br>32768: from 10.0.1.5 lookup 16<br>32769: from 10.0.2.3 lookup 16<br>167772417: from 10.0.1.1/24 lookup 167772417<br>167772417: from 10.0.1.1/24 lookup 167772417<br>167772673: from 10.0.2.1/24 lookup 167772673</p><p>在main表中没有合适的路由：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip route list table main<br>10.0.1.0/24 dev qr-ddbdc784-d7 proto kernel scope link src 10.0.1.1<br>10.0.2.0/24 dev qr-001d0ed9-01 proto kernel scope link src 10.0.2.1<br>169.254.31.28/31 dev rfp-0fbb351e-a proto kernel scope link src 169.254.31.28</p><p>由于包是从10.0.1.5发来的之后会查看table 16:<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip route list table 16<br>default via 169.254.31.29 dev rfp-0fbb351e-a<br>包会命中这条路由。</p><p>路由之后会通过netfilter的POSTROUTING链中进行SNAT：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa iptables -nvL -t nat<br>。。。<br>Chain neutron-l3-agent-float-snat (1 references)<br> pkts bytes target prot opt in out source destination<br>    0 0 SNAT all – <em> </em> 10.0.2.3 0.0.0.0/0 to:172.24.4.7<br>    0 0 SNAT all – <em> </em> 10.0.1.5 0.0.0.0/0 to:172.24.4.5<br>。。。</p><p>之后就可以看到包会通过rfp-0fbb351e-a发送给169.254.31.29。</p><p>端口rfp-0fbb351e-a和fpr-0fbb351e-a是一对veth pair。在fip namespace中你可以看到这个接口：<br>root@dvr-compute1:~# ip netns exec fip-fbd46644-c70f-4227-a414-862a00cbd1d2 ifconfig<br>fg-081d537b-06 Link encap:Ethernet  HWaddr fa:16:3e:a4:eb:6b<br>          inet addr:172.24.4.6  Bcast:172.24.4.255  Mask:255.255.255.0<br>          inet6 addr: fe80::f816:3eff:fea4:eb6b/64 Scope:Link<br>          UP BROADCAST RUNNING  MTU:1500  Metric:1<br>          RX packets:0 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:50 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:0<br>          RX bytes:0 (0.0 B)  TX bytes:2512 (2.5 KB)</p><p>fpr-0fbb351e-a Link encap:Ethernet  HWaddr 42:0d:9f:49:63:c6<br>          inet addr:169.254.31.29  Bcast:0.0.0.0  Mask:255.255.255.254<br>          inet6 addr: fe80::400d:9fff:fe49:63c6/64 Scope:Link<br>          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1<br>          RX packets:12 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:12 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:1000<br>          RX bytes:1116 (1.1 KB)  TX bytes:1116 (1.1 KB)</p><p>lo        Link encap:Local Loopback<br>          inet addr:127.0.0.1  Mask:255.0.0.0<br>          inet6 addr: ::1/128 Scope:Host<br>          UP LOOPBACK RUNNING  MTU:65536  Metric:1<br>          RX packets:13 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:13 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:0<br>          RX bytes:1250 (1.2 KB)  TX bytes:1250 (1.2 KB)</p><p>到了fip的namespace之后，会查询路由， 这里有通往公网的默认路由：<br>root@dvr-compute1:~# ip netns exec fip-fbd46644-c70f-4227-a414-862a00cbd1d2 ip route<br>default via 172.24.4.1 dev fg-081d537b-06<br>169.254.31.28/31 dev fpr-0fbb351e-a proto kernel scope link src 169.254.31.29<br>172.24.4.0/24 dev fg-081d537b-06 proto kernel scope link src 172.24.4.6<br>172.24.4.5 via 169.254.31.28 dev fpr-0fbb351e-a<br>172.24.4.7 via 169.254.31.28 dev fpr-0fbb351e-a</p><p>通过fg-081d537b-06 发送到br-ex。这是从虚机发送到公网的过程。</p><p>反过来，从外网发起连接到虚机时，在fip的namespace会做arp代理：<br>root@dvr-compute1:~# ip netns exec fip-fbd46644-c70f-4227-a414-862a00cbd1d2 sysctl net.ipv4.conf.fg-081d537b-06.proxy_arp<br>net.ipv4.conf.fg-081d537b-06.proxy_arp = 1</p><p>可以看到接口的arp代理是打开的，对于floating ip 有以下两条路由：<br>root@dvr-compute1:~# ip netns exec fip-fbd46644-c70f-4227-a414-862a00cbd1d2 ip route<br>。。。<br>172.24.4.5 via 169.254.31.28 dev fpr-0fbb351e-a<br>172.24.4.7 via 169.254.31.28 dev fpr-0fbb351e-a<br>。。。</p><p>ARP会去通过VETH Pair到IR的namespace中去查询，在IR中可以看到，接口rfp-0fbb351e-a配置了floating ip:<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip addr<br>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default<br>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00<br>    inet 127.0.0.1/8 scope host lo<br>       valid_lft forever preferred_lft forever<br>    inet6 ::1/128 scope host<br>       valid_lft forever preferred_lft forever<br>2: rfp-0fbb351e-a: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000<br>    link/ether ea:5c:56:9a:36:9c brd ff:ff:ff:ff:ff:ff<br>    inet 169.254.31.28/31 scope global rfp-0fbb351e-a<br>       valid_lft forever preferred_lft forever<br>    inet 172.24.4.5/32 brd 172.24.4.5 scope global rfp-0fbb351e-a<br>       valid_lft forever preferred_lft forever<br>    inet 172.24.4.7/32 brd 172.24.4.7 scope global rfp-0fbb351e-a<br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::e85c:56ff:fe9a:369c/64 scope link<br>       valid_lft forever preferred_lft forever<br>17: qr-ddbdc784-d7: &lt;BROADCAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default<br>    link/ether fa:16:3e:66:13:af brd ff:ff:ff:ff:ff:ff<br>    inet 10.0.1.1/24 brd 10.0.1.255 scope global qr-ddbdc784-d7<br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::f816:3eff:fe66:13af/64 scope link<br>       valid_lft forever preferred_lft forever<br>19: qr-001d0ed9-01: &lt;BROADCAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default<br>    link/ether fa:16:3e:69:b4:05 brd ff:ff:ff:ff:ff:ff<br>    inet 10.0.2.1/24 brd 10.0.2.255 scope global qr-001d0ed9-01<br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::f816:3eff:fe69:b405/64 scope link<br>       valid_lft forever preferred_lft forever</p><p>因此fip的namespace会对这两个floating ip进行ARP回应。</p><p>外部发起目标地址为floating ip的请求后，fip会将其转发到IR中，IR的RPOROUTING链中规则如下：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa iptables -nvL -t nat<br>。。。<br>Chain neutron-l3-agent-PREROUTING (1 references)<br> pkts bytes target prot opt in out source destination<br>    0 0 REDIRECT tcp – <em> </em> 0.0.0.0/0 169.254.169.254 tcp dpt:80 redir ports 9697<br>    0 0 DNAT all – <em> </em> 0.0.0.0/0 172.24.4.7 to:10.0.2.3<br>    0 0 DNAT all – <em> </em> 0.0.0.0/0 172.24.4.5 to:10.0.1.5<br>。。。</p><p>这条DNAT规则会将floating ip地址转换为内部地址，之后进行路由查询：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip route<br>10.0.1.0/24 dev qr-ddbdc784-d7 proto kernel scope link src 10.0.1.1<br>10.0.2.0/24 dev qr-001d0ed9-01 proto kernel scope link src 10.0.2.1<br>169.254.31.28/31 dev rfp-0fbb351e-a proto kernel scope link src 169.254.31.28</p><p>目的地址是10.0.1.0/24网段的，因此会从qr-ddbdc784-d7转发出去。之后就会转发到br-int再到虚机。</p><p>第三种情况 – 南北向流量(虚机没有floating ip)<br>在虚机没有floating ip的情况下，从虚机发出的包会首先到IR，IR中查询路由：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip rule<br>0: from all lookup local<br>32766: from all lookup main<br>32767: from all lookup default<br>32768: from 10.0.1.5 lookup 16<br>32769: from 10.0.2.3 lookup 16<br>167772417: from 10.0.1.1/24 lookup 167772417<br>167772417: from 10.0.1.1/24 lookup 167772417<br>167772673: from 10.0.2.1/24 lookup 167772673</p><p>会先查询main表，之后查询167772417表。<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip route list table 167772417<br>default via 10.0.1.6 dev qr-ddbdc784-d7</p><p>这个表会将其转发给10.0.1.6,而这个IP就是在network node上的router_centralized_snat接口。</p><p>在network node的snat namespace中，我们可以看到这个接口：<br>stack@dvr-controller:/root$ sudo ip netns exec snat-0fbb351e-a65b-4790-a409-8fb219ce16aa ifconfig<br>lo        Link encap:Local Loopback<br>          inet addr:127.0.0.1  Mask:255.0.0.0<br>          inet6 addr: ::1/128 Scope:Host<br>          UP LOOPBACK RUNNING  MTU:65536  Metric:1<br>          RX packets:0 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:0<br>          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)</p><p>qg-4d15b7f6-cb Link encap:Ethernet  HWaddr fa:16:3e:24:0b:6b<br>          inet addr:172.24.4.4  Bcast:172.24.4.255  Mask:255.255.255.0<br>          inet6 addr: fe80::f816:3eff:fe24:b6b/64 Scope:Link<br>          UP BROADCAST RUNNING  MTU:1500  Metric:1<br>          RX packets:5 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:144 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:0<br>          RX bytes:210 (210.0 B)  TX bytes:13320 (13.3 KB)</p><p>sg-427653e4-a3 Link encap:Ethernet  HWaddr fa:16:3e:9f:55:67<br>          inet addr:10.0.1.6  Bcast:10.0.1.255  Mask:255.255.255.0<br>          inet6 addr: fe80::f816:3eff:fe9f:5567/64 Scope:Link<br>          UP BROADCAST RUNNING  MTU:1500  Metric:1<br>          RX packets:167 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:52 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:0<br>          RX bytes:16260 (16.2 KB)  TX bytes:4460 (4.4 KB)</p><p>sg-5df1ec71-d3 Link encap:Ethernet  HWaddr fa:16:3e:13:55:66<br>          inet addr:10.0.2.2  Bcast:10.0.2.255  Mask:255.255.255.0<br>          inet6 addr: fe80::f816:3eff:fe13:5566/64 Scope:Link<br>          UP BROADCAST RUNNING  MTU:1500  Metric:1<br>          RX packets:34 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:12 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:0<br>          RX bytes:3412 (3.4 KB)  TX bytes:952 (952.0 B)</p><p>stack@dvr-controller:/root$ sudo ip netns exec snat-0fbb351e-a65b-4790-a409-8fb219ce16aa iptables -nvL -t nat<br>。。。<br>Chain neutron-l3-agent-snat (1 references)<br> pkts bytes target prot opt in out source destination<br>    0 0 SNAT all – <em> </em> 10.0.1.0/24 0.0.0.0/0 to:172.24.4.4<br>    0 0 SNAT all – <em> </em> 10.0.2.0/24 0.0.0.0/0 to:172.24.4.4<br>。。。</p><p>这里就和以前的L3类似，会将没有floating ip的包SNAT成一个172.24.4.4(DVR的网关臂)。这个过程是和以前L3类似的，不再累述。</p><p>stack@dvr-controller:/root$ sudo ip netns exec snat-0fbb351e-a65b-4790-a409-8fb219ce16aa iptables -nvL -t nat<br>。。。<br>Chain neutron-l3-agent-snat (1 references)<br> pkts bytes target prot opt in out source destination<br>    0 0 SNAT all – <em> </em> 10.0.1.0/24 0.0.0.0/0 to:172.24.4.4<br>    0 0 SNAT all – <em> </em> 10.0.2.0/24 0.0.0.0/0 to:172.24.4.4<br>。。。</p><p>这里就和以前的L3类似，会将没有floating ip的包SNAT成一个172.24.4.4(DVR的网关臂)。这个过程是和以前L3类似的，不再累述。</p><hr><p>原文：<a href="https://blog.csdn.net/matt_mao/article/details/39180135" target="_blank" rel="noopener">https://blog.csdn.net/matt_mao/article/details/39180135</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章总结的很好了，偷懒直接转过来，以便日后不时查看。&lt;/p&gt;
&lt;p&gt;首先看一下，没有使用DVR的问题在哪里：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/chendave/chendave.github.io/raw/master/css/i
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>Python基础-__name__ == &#39;__main__&#39;</title>
    <link href="http://jungler.com/2018/10/06/Python%E5%9F%BA%E7%A1%80-name-main/"/>
    <id>http://jungler.com/2018/10/06/Python基础-name-main/</id>
    <published>2018-10-06T07:44:32.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p>今年国庆节过的比较悠闲，中间只有一天来了一个短途去了苏州，其余大部分时间都是在家休息，这是我第二次去苏州，印象中对苏州园林的印象还一直来自于中学课本，园林应该是一个大的公园，有竹林，小桥，流水，假山，凉亭。算得上是一个自然景观吧。这次去亲身体验发现我的假设大部分还是对的，只是它不是一座公园，而是有钱人的私家宅院。读万卷书，行万里路，可见没有实践的永远不能保证是完全正确的，想象和现实总是有着各种差距。</p><p>看了Pence的演讲，即便我无意于或者不忍于贬低自己的祖国，但却又无法说服自己这些个烂疮实实在在的存在于这个国度。无奈，无力，失语。但愿正如Pence引用的那句话，Heaven not only see the future, but also give us the hope!</p><hr><p>回过头来看看Python为什么需要__name__ == ‘__main__’？ 先看看两段代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># executor.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entry</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"implement something here..."</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"do something here..."</span>)</span><br><span class="line"><span class="keyword">print</span> __name__</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># caller.py</span></span><br><span class="line"><span class="keyword">import</span> executor</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>如果直接运行第一段程序，我们将得到下面的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;python executor.py</span><br><span class="line"><span class="keyword">do</span> something here...</span><br><span class="line">__main__</span><br></pre></td></tr></table></figure><p>运行第二段程序输出下面的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;python caller.py</span><br><span class="line"><span class="keyword">do</span> something here...</span><br><span class="line">executor</span><br></pre></td></tr></table></figure><p>这里我们得出两条结论：<br>1）Python里可以直接在模块里写不属于任何function的语句，例如第一段程序里的两个print语句，这个有点类似shell，而不同于java或者c语言，这些语句无论在本模块被执行或者被import到其它python文件中去，都会被自动执行，这有时不是我们希望看到的。而实际上这些语句常常被理解为Python的main函数。我们可以忽略所谓的main函数，因为这对于我们的理解无益，但是我们可以将其改写为下面的形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># executor.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entry</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"implement something here..."</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"do something here..."</span>)</span><br><span class="line">    <span class="keyword">print</span> __name__</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>这样以第一种方式直接执行此文件，得到的将是相同的结果。</p><p>2）__name__被解释为不同的名字，第一种方式被解释为__main__, 而第二种被解释为executor，所以当我们将程序改写为上面的形式的时候，main中的定义的语句这次将得不到执行。这时候将没有任何输出。但同时可以通过显示的调用<em>executor.main()</em>来让这两行语句得到执行。</p><p>所以，python程序中if __name__ == ‘__main__’ 这行语句的目的就是希望有些语句能有条件的得到执行，或者说只有本模块直接运行时才会被调用。这样保证了此模块被其它模块引入时这些语句不会被调用，即便它时所谓的main函数。<br>那什么时候需要用这样的语句呢？一个典型的例子比如在做模块的单元测试的时候。</p><hr><p>[1] <a href="https://interactivepython.org/runestone/static/CS152f17/Functions/mainfunction.html" target="_blank" rel="noopener">https://interactivepython.org/runestone/static/CS152f17/Functions/mainfunction.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今年国庆节过的比较悠闲，中间只有一天来了一个短途去了苏州，其余大部分时间都是在家休息，这是我第二次去苏州，印象中对苏州园林的印象还一直来自于中学课本，园林应该是一个大的公园，有竹林，小桥，流水，假山，凉亭。算得上是一个自然景观吧。这次去亲身体验发现我的假设大部分还是对的，只
      
    
    </summary>
    
    
      <category term="Python" scheme="http://jungler.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>系统重启后恢复OpenStack网络设置-tips</title>
    <link href="http://jungler.com/2018/09/24/%E7%B3%BB%E7%BB%9F%E9%87%8D%E5%90%AF%E5%90%8E%E6%81%A2%E5%A4%8DOpenStack%E7%BD%91%E7%BB%9C%E8%AE%BE%E7%BD%AE-tips/"/>
    <id>http://jungler.com/2018/09/24/系统重启后恢复OpenStack网络设置-tips/</id>
    <published>2018-09-24T09:22:59.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p>眼看着今天就要过去了，一个月就要过去了，马上一年也就要过去了，可你又能怎样？<br>昨天在焦虑，今天还在焦虑，明天将继续焦虑，何时能停止？<br>既然无力挣扎，那就闭着眼睛过吧，时间最终会给我们答案，尘过尘，土归土，看谈一些就好，看空一些就好。</p><p>不管OpenStack是不是还有些把玩的价值，但终归割舍不下，我一直把它看作一本书，一本可以提升自己的书，至于它能带给你什么？ Who knows? Who cares?</p><p>所以，当实验室里每次断电之后，虚拟网络都将无法工作，我还是会继续去<em>stack</em>一下，终于有一天我无法再次忍受，我要去看个究竟，当愚钝的翻过一行一行脚本之后，我好像找到了答案。</p><p>先看看<em>lib/neutron_plugins/services/l3</em>里的这个函数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> _configure_neutron_l3_agent &#123;</span><br><span class="line"></span><br><span class="line">    cp <span class="variable">$NEUTRON_DIR</span>/etc/l3_agent.ini.sample <span class="variable">$Q_L3_CONF_FILE</span></span><br><span class="line"></span><br><span class="line">    iniset <span class="variable">$Q_L3_CONF_FILE</span> DEFAULT debug <span class="variable">$ENABLE_DEBUG_LOG_LEVEL</span></span><br><span class="line">    iniset <span class="variable">$Q_L3_CONF_FILE</span> AGENT root_helper <span class="string">"<span class="variable">$Q_RR_COMMAND</span>"</span></span><br><span class="line">    <span class="keyword">if</span> [[ <span class="string">"<span class="variable">$Q_USE_ROOTWRAP_DAEMON</span>"</span> == <span class="string">"True"</span> ]]; <span class="keyword">then</span></span><br><span class="line">        iniset <span class="variable">$Q_L3_CONF_FILE</span> AGENT root_helper_daemon <span class="string">"<span class="variable">$Q_RR_DAEMON_COMMAND</span>"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">    _neutron_setup_interface_driver <span class="variable">$Q_L3_CONF_FILE</span></span><br><span class="line"></span><br><span class="line">    neutron_plugin_configure_l3_agent <span class="variable">$Q_L3_CONF_FILE</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># If we've given a PUBLIC_INTERFACE to take over, then we assume</span></span><br><span class="line">    <span class="comment"># that we can own the whole thing, and privot it into the OVS</span></span><br><span class="line">    <span class="comment"># bridge. If we are not, we're probably on a single interface</span></span><br><span class="line">    <span class="comment"># machine, and we just setup NAT so that fixed guests can get out.</span></span><br><span class="line">    <span class="keyword">if</span> [[ -n <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> ]]; <span class="keyword">then</span>   <span class="comment"># *看这里!* </span></span><br><span class="line">        _move_neutron_addresses_route <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> <span class="string">"<span class="variable">$OVS_PHYSICAL_BRIDGE</span>"</span> True False <span class="string">"inet"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> [[ $(ip -f inet6 a s dev <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> | grep -c <span class="string">'global'</span>) != 0 ]]; <span class="keyword">then</span></span><br><span class="line">            _move_neutron_addresses_route <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> <span class="string">"<span class="variable">$OVS_PHYSICAL_BRIDGE</span>"</span> False False <span class="string">"inet6"</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> <span class="variable">$default_v4_route_devs</span>; <span class="keyword">do</span></span><br><span class="line">            sudo iptables -t nat -A POSTROUTING -o <span class="variable">$d</span> -s <span class="variable">$FLOATING_RANGE</span> -j MASQUERADE</span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从这个函数可以看出，如果绑定了某一个物理网卡（例如在<em>localrc</em>中配置了”PUBLIC_INTERFACE”），那么将会调用”_move_neutron_addresses_route”来做进一步处理，否则就做一个源地址伪装（MASQUERADE）就算完了。</p><p>核心就是这段代码了，<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">_move_neutron_addresses_route <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> <span class="string">"<span class="variable">$OVS_PHYSICAL_BRIDGE</span>"</span> True False <span class="string">"inet"</span></span><br></pre></td></tr></table></figure></p><p>来重点看下绑定物理网卡的处理方式，函数有点长，我们调重点的看一下部分核心代码，<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lib/neutron-legacy</span></span><br><span class="line"><span class="comment"># _move_neutron_addresses_route() - Move the primary IP to the OVS bridge</span></span><br><span class="line"><span class="comment"># on startup, or back to the public interface on cleanup. If no IP is</span></span><br><span class="line"><span class="comment"># configured on the interface, just add it as a port to the OVS bridge.</span></span><br><span class="line"><span class="keyword">function</span> _move_neutron_addresses_route &#123;</span><br><span class="line">...</span><br><span class="line">    <span class="keyword">if</span> [[ -n <span class="string">"<span class="variable">$from_intf</span>"</span> &amp;&amp; -n <span class="string">"<span class="variable">$to_intf</span>"</span> ]]; <span class="keyword">then</span></span><br><span class="line">...</span><br><span class="line">        DEFAULT_ROUTE_GW=$(ip -f <span class="variable">$af</span> r | awk <span class="string">"/default.+<span class="variable">$from_intf</span>\s/ &#123; print \$3; exit &#125;"</span>) </span><br><span class="line">        IP_BRD=$(ip -f <span class="variable">$af</span> a s dev <span class="variable">$from_intf</span> scope global primary | grep inet | awk <span class="string">'&#123; print $2, $3, $4; exit &#125;'</span>) <span class="comment">#①</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> [ <span class="string">"<span class="variable">$DEFAULT_ROUTE_GW</span>"</span> != <span class="string">""</span> ]; <span class="keyword">then</span></span><br><span class="line">            ADD_DEFAULT_ROUTE=<span class="string">"sudo ip -f <span class="variable">$af</span> r replace default via <span class="variable">$DEFAULT_ROUTE_GW</span> dev <span class="variable">$to_intf</span>"</span> <span class="comment"># ②</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> [[ <span class="string">"<span class="variable">$add_ovs_port</span>"</span> == <span class="string">"True"</span> ]]; <span class="keyword">then</span></span><br><span class="line">            ADD_OVS_PORT=<span class="string">"sudo ovs-vsctl --may-exist add-port <span class="variable">$to_intf</span> <span class="variable">$from_intf</span>"</span> <span class="comment"># ③</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">...</span><br><span class="line">        <span class="keyword">if</span> [[ <span class="string">"<span class="variable">$IP_BRD</span>"</span> != <span class="string">""</span> ]]; <span class="keyword">then</span></span><br><span class="line">            IP_DEL=<span class="string">"sudo ip addr del <span class="variable">$IP_BRD</span> dev <span class="variable">$from_intf</span>"</span> <span class="comment"># ④</span></span><br><span class="line">            IP_REPLACE=<span class="string">"sudo ip addr replace <span class="variable">$IP_BRD</span> dev <span class="variable">$to_intf</span>"</span> <span class="comment"># ⑤ </span></span><br><span class="line">            IP_UP=<span class="string">"sudo ip link set <span class="variable">$to_intf</span> up"</span> <span class="comment"># ⑥</span></span><br><span class="line">            <span class="keyword">if</span> [[ <span class="string">"<span class="variable">$af</span>"</span> == <span class="string">"inet"</span> ]]; <span class="keyword">then</span></span><br><span class="line">                IP=$(<span class="built_in">echo</span> <span class="variable">$IP_BRD</span> | awk <span class="string">'&#123; print $1; exit &#125;'</span> | grep -o -E <span class="string">'(.*)/'</span> | cut -d <span class="string">"/"</span> -f1)</span><br><span class="line">                ARP_CMD=<span class="string">"arping -A -c 3 -w 4.5 -I <span class="variable">$to_intf</span> <span class="variable">$IP</span> "</span> <span class="comment"># ⑦</span></span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">...</span><br><span class="line">        <span class="variable">$DEL_OVS_PORT</span>; <span class="variable">$IP_DEL</span>; <span class="variable">$IP_REPLACE</span>; <span class="variable">$IP_UP</span>; <span class="variable">$ADD_OVS_PORT</span>; <span class="variable">$ADD_DEFAULT_ROUTE</span>; <span class="variable">$ARP_CMD</span></span><br></pre></td></tr></table></figure></p><p>先来看下网络的路由配置情况，<em>eno2</em>将会作为<em>PUBLIC_INTERFACE</em>被绑定到<em>br-ex</em>上。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ip route</span><br><span class="line">default via 192.168.18.1 dev eno2  proto static  metric 100</span><br><span class="line">169.254.0.0/16 dev docker0  scope link  metric 1000 linkdown</span><br><span class="line">172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1 linkdown</span><br><span class="line">192.168.16.0/21 dev eno2  proto kernel  scope link  src 192.168.18.24  metric 100</span><br></pre></td></tr></table></figure><ol><li><p>这两句是获取系统当前的一些网络配置，<em>DEFAULT_ROUTE_GW</em>是物理机的默认路由，<em>IP_BRD</em>得到的是物理网卡上的主IP地址配置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ip -f inet r | awk <span class="string">"/default.+eno2\s/ &#123; print \$3; exit &#125;"</span></span><br><span class="line">192.168.18.1</span><br><span class="line"></span><br><span class="line">$ ip -f inet a s dev eno2 scope global primary | grep inet | awk <span class="string">'&#123; print $2, $3, $4; exit &#125;'</span></span><br><span class="line">192.168.18.24/21 brd 192.168.23.255</span><br></pre></td></tr></table></figure></li><li><p>将默认路由替换为目标网卡，这里当然是替换为OVS的bridge <em>br-ex</em>，这一步之后默认的路由的IP地址虽然没变，但是device已经改为<em>br-ex</em>了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip -f inet r replace default via 192.168.18.1 dev br-ex</span><br></pre></td></tr></table></figure></li><li><p>将物理网卡绑定到<em>br-ex</em>上。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ovs-vsctl --may-exist add-port br-ex eno2</span><br></pre></td></tr></table></figure></li><li><p>这一步将物理网卡上的主IP地址删除，之后该地址将配置到<em>br-ex</em>上。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip addr del 192.168.18.24/21 brd 192.168.23.255 dev eno2</span><br></pre></td></tr></table></figure></li><li><p>将物理网卡上的主IP地址配置到<em>br-ex</em>上。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip addr replace 192.168.18.24/21 brd 192.168.23.255 dev br-ex</span><br></pre></td></tr></table></figure></li><li><p>不解释</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip link <span class="built_in">set</span> br-ex up</span><br></pre></td></tr></table></figure></li><li><p>设置ARP请求的一些参数，arping没用过，具体在干什么，还是不太了解。</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ arping -A -c 3 -w 4.5 -I br-ex 192.168.18.24</span><br></pre></td></tr></table></figure><p>之后就是顺序执行这几个命令了，一句话来解释这段代码干的事情就是这段注释:</p><blockquote><p>Move the primary IP to the OVS bridge on startup, or back to the<br>public interface on cleanup. If no IP is configured on the interface,<br>just add it as a port to the OVS bridge.</p></blockquote><p>如果系统reboot了，网络挂了，不行就顺序再来一便吧，但问题是为什么网络不能自动恢复？这是个大问题啊？！</p><p>希望国庆节之后能对DVR来做个了结吧。</p><p>p.s. 天又晚了，码字确实费时间。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;眼看着今天就要过去了，一个月就要过去了，马上一年也就要过去了，可你又能怎样？&lt;br&gt;昨天在焦虑，今天还在焦虑，明天将继续焦虑，何时能停止？&lt;br&gt;既然无力挣扎，那就闭着眼睛过吧，时间最终会给我们答案，尘过尘，土归土，看谈一些就好，看空一些就好。&lt;/p&gt;
&lt;p&gt;不管OpenS
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>DevStack激活DVR相关配置</title>
    <link href="http://jungler.com/2018/08/11/DevStack%E6%BF%80%E6%B4%BBDVR%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE/"/>
    <id>http://jungler.com/2018/08/11/DevStack激活DVR相关配置/</id>
    <published>2018-08-11T08:57:25.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p>这些日子脑子里经常会想到《可可西里》电影里的一个画面，从刚陷入到流沙时的垂死挣扎越陷越深到后来彻底绝望放弃，电影表达了个人在恶劣的自然环境下的无能无力，换个角度想想，我们绝大多数人何尝不是已经陷入了深不见底的绝望之中，只不过这里不是沙漠戈壁而是同样现实的社会，我们挣扎着，不想认命，不想就此结束，但是结果常常是在岁月的流逝中，一步步走向属于我们这代人的结局。</p><p>DVR(Distributed virtual router)已经理解的差不多了，一句话来总结DVR就是DVR可以网络流量的负载均衡，以解决过往所有流量需要网络节点参与从而可能造成性能瓶颈的问题。这里分享一下如何在DevStack中做配置来激活DVR，</p><ul><li><p>controller节点(网络节点)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Base configuration</span></span><br><span class="line">HOST_IP=192.168.20.132</span><br><span class="line">SERVICE_HOST=192.168.20.132</span><br><span class="line">MYSQL_HOST=<span class="variable">$SERVICE_HOST</span></span><br><span class="line">RABBIT_HOST=<span class="variable">$SERVICE_HOST</span></span><br><span class="line">GLANCE_HOSTPORT=<span class="variable">$SERVICE_HOST</span>:9292</span><br><span class="line">ADMIN_PASSWORD=abc123</span><br><span class="line">DATABASE_PASSWORD=abc123</span><br><span class="line">RABBIT_PASSWORD=abc123</span><br><span class="line">SERVICE_PASSWORD=abc123</span><br><span class="line">DATABASE_TYPE=mysql</span><br><span class="line"></span><br><span class="line"><span class="comment"># Neutron options</span></span><br><span class="line">Q_USE_SECGROUP=True</span><br><span class="line">FLOATING_RANGE=<span class="string">"192.168.23.1/21"</span></span><br><span class="line">Q_FLOATING_ALLOCATION_POOL=start=192.168.23.120,end=192.168.23.150</span><br><span class="line">IPV4_ADDRS_SAFE_TO_USE=<span class="string">"10.0.0.0/22"</span></span><br><span class="line">PUBLIC_NETWORK_GATEWAY=<span class="string">"192.168.18.1"</span></span><br><span class="line">PUBLIC_INTERFACE=eno1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Open vSwitch provider networking configuration</span></span><br><span class="line">Q_USE_PROVIDERNET_FOR_PUBLIC=True</span><br><span class="line">OVS_PHYSICAL_BRIDGE=br-ex</span><br><span class="line">PUBLIC_BRIDGE=br-ex</span><br><span class="line">OVS_BRIDGE_MAPPINGS=public:br-ex</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multi-node cluster</span></span><br><span class="line">MULTI_HOST=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># MISC</span></span><br><span class="line">LOGFILE=/opt/stack/logs/stack.sh.log</span><br><span class="line">NOVA_VNC_ENABLED=True</span><br><span class="line">NOVNCPROXY_URL=<span class="string">"http://<span class="variable">$SERVICE_HOST</span>:6080/vnc_auto.html"</span></span><br><span class="line">VNCSERVER_LISTEN=<span class="variable">$HOST_IP</span></span><br><span class="line">VNCSERVER_PROXYCLIENT_ADDRESS=<span class="variable">$VNCSERVER_LISTEN</span></span><br><span class="line">GIT_BASE=https://git.openstack.org</span><br><span class="line"></span><br><span class="line"><span class="comment"># Settings for DVR networking, DVR depends on vxlan and ml2/ovs,</span></span><br><span class="line"><span class="comment"># this is not verified in the latest version.</span></span><br><span class="line"><span class="comment"># The setting on the network node.</span></span><br><span class="line">Q_DVR_MODE=dvr_snat</span><br><span class="line">Q_PLUGIN=ml2</span><br><span class="line">Q_ML2_TENANT_NETWORK_TYPE=vxlan</span><br></pre></td></tr></table></figure></li><li><p>计算节点（这里需要注意<strong>ENABLED_SERVICES</strong>中需要将neutron相关的服务都加上，DHCP服务除外）</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Basic configuration</span></span><br><span class="line"><span class="comment"># Reference: https://docs.openstack.org/devstack/latest/guides/neutron.html</span></span><br><span class="line">HOST_IP=192.168.18.79</span><br><span class="line">SERVICE_HOST=192.168.20.132</span><br><span class="line">MYSQL_HOST=<span class="variable">$SERVICE_HOST</span></span><br><span class="line">RABBIT_HOST=<span class="variable">$SERVICE_HOST</span></span><br><span class="line">GLANCE_HOSTPORT=<span class="variable">$SERVICE_HOST</span>:9292</span><br><span class="line">ADMIN_PASSWORD=abc123</span><br><span class="line">DATABASE_PASSWORD=abc123</span><br><span class="line">MYSQL_PASSWORD=abc123</span><br><span class="line">RABBIT_PASSWORD=abc123</span><br><span class="line">SERVICE_PASSWORD=abc123</span><br><span class="line">DATABASE_TYPE=mysql</span><br><span class="line">MULTI_HOST=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Neutron options</span></span><br><span class="line">FLOATING_RANGE=<span class="string">"192.168.23.1/21"</span></span><br><span class="line">Q_FLOATING_ALLOCATION_POOL=start=192.168.23.120,end=192.168.23.150</span><br><span class="line">IPV4_ADDRS_SAFE_TO_USE=<span class="string">"10.0.0.0/22"</span></span><br><span class="line">PUBLIC_NETWORK_GATEWAY=<span class="string">"192.168.18.1"</span></span><br><span class="line">PUBLIC_INTERFACE=eno1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Services that a compute node runs</span></span><br><span class="line">ENABLED_SERVICES=n-cpu,q-agt,n-api-meta,c-vol,placement-client,placement-api,neutron,q-l3,q-meta</span><br><span class="line"></span><br><span class="line"><span class="comment"># Misc configuration</span></span><br><span class="line">LOGFILE=/opt/stack/logs/stack.sh.log</span><br><span class="line">NOVA_VNC_ENABLED=True</span><br><span class="line">NOVNCPROXY_URL=<span class="string">"http://<span class="variable">$SERVICE_HOST</span>:6080/vnc_auto.html"</span></span><br><span class="line">VNCSERVER_LISTEN=<span class="variable">$HOST_IP</span></span><br><span class="line">VNCSERVER_PROXYCLIENT_ADDRESS=<span class="variable">$VNCSERVER_LISTEN</span></span><br><span class="line">GIT_BASE=https://git.openstack.org</span><br><span class="line"></span><br><span class="line"><span class="comment"># Settings for DVR networking</span></span><br><span class="line">Q_DVR_MODE=dvr</span><br><span class="line">Q_PLUGIN=ml2</span><br><span class="line">Q_ML2_TENANT_NETWORK_TYPE=vxlan</span><br></pre></td></tr></table></figure><p>这里或许有部分参数是可选的，但是加上也没有关系。</p><hr><p>[1] <a href="https://github.com/chendave/initrepo/tree/master/openstack/localrc/dvr" target="_blank" rel="noopener">https://github.com/chendave/initrepo/tree/master/openstack/localrc/dvr</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这些日子脑子里经常会想到《可可西里》电影里的一个画面，从刚陷入到流沙时的垂死挣扎越陷越深到后来彻底绝望放弃，电影表达了个人在恶劣的自然环境下的无能无力，换个角度想想，我们绝大多数人何尝不是已经陷入了深不见底的绝望之中，只不过这里不是沙漠戈壁而是同样现实的社会，我们挣扎着，不
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>Neutron接口命名规则</title>
    <link href="http://jungler.com/2018/07/28/neutron%E6%8E%A5%E5%8F%A3%E5%91%BD%E5%90%8D%E6%BD%9C%E8%A7%84%E5%88%99/"/>
    <id>http://jungler.com/2018/07/28/neutron接口命名潜规则/</id>
    <published>2018-07-28T11:38:12.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p>这些日子上海挺热的，前些日子去杭州才发现，杭州比上海还要热个几度，大热天爆晒36+，带着老婆孩子去西溪湿地去暴走，这个天当然不适合旅游，只是往年都要找个日子去杭州来个短游，今年正好得去杭州去拜访吉利集团，可以理解为为了所谓的旅游而旅游吧。</p><p><em>Neutron</em>已经看了有些日子了，计划不久的将来对有无DVR情况下南北与东西流量做个总结，当作一个铺垫吧，这里对Neutron里的网络接口命名做个小结，当看到<em>tap, qbr, qvb, qvo, qr-, qg-, br</em>前缀命令的接口设备有没有一点小晕呢？其实这些设备本质上都是一样的，但是应用的场景又各不相同，不同的名称前缀代表了不同的含义，所以熟悉了之后只看这些前缀也就略知一二了。</p><p><em>tap-</em><br>这个就是tap设备，每个虚拟机都对应一个tap设备，tap设备需要挂在linux bridge上或者OVS上，OpenStack里虚拟机的tap设备挂在linux bridge上，DHCP namespace里的tap设备挂在OVS上。<br>例如下面的tap设备”tap0cf5c0e2-26”来自于DHCP namespace并挂在OVS上。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ovs-vsctl show</span><br><span class="line">    Bridge br-int</span><br><span class="line">        Controller <span class="string">"tcp:127.0.0.1:6633"</span></span><br><span class="line">            is_connected: <span class="literal">true</span></span><br><span class="line">        fail_mode: secure</span><br><span class="line">..</span><br><span class="line">        Port <span class="string">"tap0cf5c0e2-26"</span></span><br><span class="line">            tag: 1</span><br><span class="line">            Interface <span class="string">"tap0cf5c0e2-26"</span></span><br><span class="line">                <span class="built_in">type</span>: internal</span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip netns <span class="built_in">exec</span> qdhcp-2f0982cf-3f10-4ae5-96de-1e70d289fbf0 ip a</span><br><span class="line">64: tap0cf5c0e2-26: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:fb:9b:53 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.0.0.2/26 brd 10.0.0.63 scope global tap0cf5c0e2-26</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fd7d:9d2b:8fb7:0:f816:3eff:fefb:9b53/64 scope global</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p><em>qvb-</em>，<em>qvo-</em>与<em>qbr-</em><br>qvb与qvo是一对veth pair，可以在系统上看到这一对veth pair，其中qvb设备挂在linux bridge上，qvo设备挂在OVS上。<br>我们可以通过在系统上输入ip a命令来查看这些veth pair的信息，例如我的系统上可以看到下面的设备：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">70: qvo285c68b1-9d@qvb285c68b1-9d: &lt;BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master ovs-system state UP group default qlen 1000</span><br></pre></td></tr></table></figure></p><p>qbr用来定义命名一个linux bridge。</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/linux-bridge.png" alt=""></p><p><em>gr-</em>与<em>qg-</em><br>qr设备用于连接租户网络（租户内部IP地址），qg设备用于连接外部网络（通过floating IP连接外部网络）。<br>例如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip netns <span class="built_in">exec</span> qrouter-3b1a4673-4ada-4988-a11b-86fcacfb0ea0 ip a</span><br><span class="line">65: qr-f937ae2f-ec: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:ac:b9:00 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.0.0.1/26 brd 10.0.0.63 scope global qr-f937ae2f-ec</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::f816:3eff:feac:b900/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">66: qg-4386c8fb-38: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:0d:5a:4d brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.42.16/24 brd 192.168.42.255 scope global qg-4386c8fb-38</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.42.11/32 brd 192.168.42.11 scope global qg-4386c8fb-38</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这些日子上海挺热的，前些日子去杭州才发现，杭州比上海还要热个几度，大热天爆晒36+，带着老婆孩子去西溪湿地去暴走，这个天当然不适合旅游，只是往年都要找个日子去杭州来个短游，今年正好得去杭州去拜访吉利集团，可以理解为为了所谓的旅游而旅游吧。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Neutro
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>虚拟机网络不通的几个思路[2]</title>
    <link href="http://jungler.com/2018/07/01/%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B8%8D%E9%80%9A%E7%9A%84%E5%87%A0%E4%B8%AA%E6%80%9D%E8%B7%AF%5B2%5D/"/>
    <id>http://jungler.com/2018/07/01/虚拟机网络不通的几个思路[2]/</id>
    <published>2018-07-01T09:14:52.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/stock.png" alt=""></p><p>2018年六月份已经过去，刚过去的两个月是入市以来回撤最大的两个月，虽然资金量不大，但依然对未来抱有希望，说不定哪天走狗屎运咸鱼翻身了呢。本质上来说，股市也好，人生中的各种选择也好，都是一种赌博，就看你的筹码有多大，未来到底看的有多清，当然我从不屑于直接参与赌博，只是厌倦了一辈子都是如此谨谨慎慎，按部就班的生活。</p><p>2018年下半年祝福自己依然有颗勇敢的心，怀抱希望的走下去，Get busy living, Or get busy dying!</p><p>回来继续做些笔记，之前提到过OpenStack 虚拟机网络不通的几个思路，这篇笔记对其中iptable的规则来做个深入的了解：</p><blockquote><p>sudo iptables -t nat -A POSTROUTING -s 192.168.42.0/24 -o eno1 -j MASQUERADE</p></blockquote><h3 id="为什么？？？"><a href="#为什么？？？" class="headerlink" title="为什么？？？"></a>为什么？？？</h3><p>首先我们假设floating IP的地址和物理机的IP地址不在同一个网段，而是一个自定义的网段（后面我会探讨不同的应用场景，比如floating IP和物理机器的IP地址在需要在同一个网段的情况如何做网络配置），比如：</p><ul><li>物理机的IP地址为: 192.168.18.0/24</li><li>floating IP地址定义为: 192.168.42.0/24<br>这里floating IP地址其实是一个不存在的网络，是需要通过OpenStack去创建的一个网络。和之前的定义不一样，这里我们只对<strong>192.168.42.0/24</strong>网段的IP进行地址伪装，伪装为eno1的IP地址出去，<strong>192.168.42.0/24</strong>是floating IP地址。如果没有上面定义的这个规则，虚拟机则无法访问外网(公司内网)，因为外部网络是不知道floating IP地址存在的，通过这个规则，从虚拟机出来的流量将得以访问外网。</li></ul><p>假设OpenStack的环境是通过DevStack安装起来的，那么你可以看到DevStack脚本已经帮你创建好了这个规则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ sudo iptables -t nat -S</span><br><span class="line">-P PREROUTING ACCEPT</span><br><span class="line">-P INPUT ACCEPT</span><br><span class="line">-P OUTPUT ACCEPT</span><br><span class="line">-P POSTROUTING ACCEPT</span><br><span class="line">-N DOCKER</span><br><span class="line">-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER</span><br><span class="line">-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER</span><br><span class="line">-A POSTROUTING -s 192.168.42.0/24 -o eno1 -j MASQUERADE  &lt;- 看这里</span><br><span class="line">-A DOCKER -i docker0 -j RETURN</span><br></pre></td></tr></table></figure><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>我们来想这样一个场景，物理机上有多个网卡，比方说<strong>eno1</strong>连接的是公司内部的网络，<strong>eno2</strong>连的是公网，通过在DevStack的localrc文件里做如下的配置：</p><blockquote><p>FLAT_INTERFACE=eno1</p></blockquote><p><strong>注：</strong></p><table><tr><td bgcolor="DarkGray">新版本中已经不建议采用nova network, 而是用neutron来提供网络服务，这样FLAT_INTERFACE将不再起作用，取而代之的是PUBLIC_INTERFACE这个属性，而这两个属性所代表的意思是不同的，参考后面的源码分析。</td></tr></table><p>DevStack会自动配置好了iptables，我们假设网络没有绑定任何物理网卡，那么应该走的是系统的默认路由，一般情况下，物理机上会配置一个默认路由，像下面这样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip route</span><br><span class="line">default via 192.168.18.1 dev eno1  proto static  metric 100</span><br><span class="line">default via 192.168.8.1 dev eno2  proto static  metric 101</span><br><span class="line">...</span><br><span class="line">192.168.16.0/21 dev eno1  proto kernel  scope link  src 192.168.20.132</span><br><span class="line">192.168.42.0/24 dev br-ex  proto kernel  scope link  src 192.168.42.1</span><br></pre></td></tr></table></figure><p>这样虚拟机通过floating IP应该可以访问公司的内网了，但是有一天，虚拟机需要访问外部的公网了，怎么办？<br>修改物理机的默认路由？例如默认路由修改为：</p><blockquote><p>default via 192.168.8.1 dev eno2  proto static  metric 100<br>default via 192.168.18.1 dev eno1  proto static  metric 101</p></blockquote><p>是否可以了呢？做个测试，在虚拟机里ping 8.8.8.8，然后通过tcpdump可以来检测eno2看的网络请求:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo tcpdump -n -i eno2 icmp</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eno2, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">18:26:57.424083 IP 192.168.42.20 &gt; 8.8.8.8: ICMP <span class="built_in">echo</span> request, id 5155, seq 1, length 64</span><br><span class="line">18:26:58.432816 IP 192.168.42.20 &gt; 8.8.8.8: ICMP <span class="built_in">echo</span> request, id 5155, seq 2, length 64</span><br><span class="line">18:26:59.440803 IP 192.168.42.20 &gt; 8.8.8.8: ICMP <span class="built_in">echo</span> request, id 5155, seq 3, length 64</span><br><span class="line">18:27:00.448842 IP 192.168.42.20 &gt; 8.8.8.8: ICMP <span class="built_in">echo</span> request, id 5155, seq 4, length 64</span><br><span class="line">18:27:01.456825 IP 192.168.42.20 &gt; 8.8.8.8: ICMP <span class="built_in">echo</span> request, id 5155, seq 5, length 64</span><br><span class="line">18:27:02.464842 IP 192.168.42.20 &gt; 8.8.8.8: ICMP <span class="built_in">echo</span> request, id 5155, seq 6, length 64</span><br></pre></td></tr></table></figure><p>虽然eno2上能检测测ping请求，但是没有reply，虚拟机无法ping通外部网络！<br>所以，手动修改iptables的规则就该上场了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo iptables -t nat -A POSTROUTING -s 192.168.42.0/24 -o eno2 -j MASQUERADE</span><br></pre></td></tr></table></figure><p>再ping一次试试：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ping 8.8.8.8</span><br><span class="line">PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.</span><br><span class="line">64 bytes from 8.8.8.8: icmp_seq=1 ttl=38 time=99.6 ms</span><br><span class="line">64 bytes from 8.8.8.8: icmp_seq=2 ttl=38 time=97.7 ms</span><br><span class="line">64 bytes from 8.8.8.8: icmp_seq=3 ttl=38 time=83.0 ms</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>看看系统上的iptables的规则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ sudo iptables -t nat -S</span><br><span class="line">-P PREROUTING ACCEPT</span><br><span class="line">-P INPUT ACCEPT</span><br><span class="line">-P OUTPUT ACCEPT</span><br><span class="line">-P POSTROUTING ACCEPT</span><br><span class="line">-N DOCKER</span><br><span class="line">-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER</span><br><span class="line">-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER</span><br><span class="line">-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE</span><br><span class="line">-A POSTROUTING -s 192.168.42.0/24 -o eno1 -j MASQUERADE</span><br><span class="line">-A POSTROUTING -s 192.168.42.0/24 -o eno2 -j MASQUERADE   &lt;- 看这里</span><br><span class="line">-A DOCKER -i docker0 -j RETURN</span><br></pre></td></tr></table></figure><p>看来虽然我们对网段<strong>192.168.42.0/24</strong>加了两个MASQUERADE规则，但它们并不互斥，这周就到这里吧。</p><h3 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h3><p>这里的源码来自于<strong>pike release</strong>版本，上面所讲的都是我们能看到的现象，我们看看代码里是如何实现的。<br>打开DevStack里这个文件，我们可以看的一清二楚：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /lib/neutron_plugins/services/l3</span></span><br><span class="line">98 default_v4_route_devs=$(ip -4 route | grep ^default | awk <span class="string">'&#123;print $5&#125;'</span>)</span><br><span class="line">...</span><br><span class="line">126     <span class="keyword">if</span> [[ -n <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> ]]; <span class="keyword">then</span></span><br><span class="line">127         _move_neutron_addresses_route <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> <span class="string">"<span class="variable">$OVS_PHYSICAL_BRIDGE</span>"</span> True False <span class="string">"inet"</span></span><br><span class="line">128</span><br><span class="line">129         <span class="keyword">if</span> [[ $(ip -f inet6 a s dev <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> | grep -c <span class="string">'global'</span>) != 0 ]]; <span class="keyword">then</span></span><br><span class="line">130             _move_neutron_addresses_route <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> <span class="string">"<span class="variable">$OVS_PHYSICAL_BRIDGE</span>"</span> False False <span class="string">"inet6"</span></span><br><span class="line">131         <span class="keyword">fi</span></span><br><span class="line">132     <span class="keyword">else</span></span><br><span class="line">133         <span class="keyword">for</span> d <span class="keyword">in</span> <span class="variable">$default_v4_route_devs</span>; <span class="keyword">do</span></span><br><span class="line">134             sudo iptables -t nat -A POSTROUTING -o <span class="variable">$d</span> -s <span class="variable">$FLOATING_RANGE</span> -j MASQUERADE</span><br><span class="line">135         <span class="keyword">done</span></span><br><span class="line">136     <span class="keyword">fi</span></span><br><span class="line">137 &#125;</span><br></pre></td></tr></table></figure></p><p>所以，当有设置这个<strong>PUBLIC_INTERFACE</strong>这个属性时，将会对应的物理网卡绑定到bridge上去，如果没有设置，则会对系统的默认路由对应的所有的所有物理网卡设备定义MASQUERADE规则，以实现虚拟机与外网访问的目的。</p><p>从这里我们可以看出，配置OpenStack需要至少配置两个不同的网络的原因所在:</p><ul><li>集群内部需要一个可以连接的外部网络，每个节点都有配置一个该网段的IP地址，网络节点需要将此网卡绑定到bridge上，这样虚拟机可以通过此网络访问外网，floating IP也可以配置在同一个网络里，这样其它物理节点可以直接访问虚拟机（比如虚拟机提供一些web服务那么可以直接在其它节点访问）。</li><li>还需要一个可以让终端用户去访问的管理机器的网络，此网络可以是内部管理网即OpenStack API服务所在的网络。<br>实际生产环境比这个要复杂的多，比如需要一个存储网络，租户网络，部署(Provision)网络等。</li></ul><p>附:</p><ol><li>查看某个table上的rules<br>sudo iptables -nL -t nat –line-number</li></ol><ol start="2"><li>删除某个规则<br>sudo iptables -D POSTROUTING 3 -t nat</li></ol><hr><p>[1] <a href="https://blog.csdn.net/chenwei8280/article/details/79601885" target="_blank" rel="noopener">https://blog.csdn.net/chenwei8280/article/details/79601885</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://github.com/chendave/chendave.github.io/raw/master/css/images/stock.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2018年六月份已经过去，刚过去的两个月是入市以来回撤最大的两个
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>PG 异常状态- active+undersized+degraded</title>
    <link href="http://jungler.com/2018/06/23/PG%E5%BC%82%E5%B8%B8%E7%8A%B6%E6%80%81-active+undersized+degraded/"/>
    <id>http://jungler.com/2018/06/23/PG异常状态-active+undersized+degraded/</id>
    <published>2018-06-23T08:53:31.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p>自己搭的3个OSD节点的集群的健康状态经常处在”WARN”状态，replicas设置为3，OSD节点数量大于3，存放的data数量也不多，<strong>ceph -s</strong> 不是期待的health ok，而是<strong>active+undersized+degraded</strong>。被这个问题困扰有段时间，因为对Ceph不太了解而一直没有找到解决方案，直到最近发邮件到社区才得到解决[1]。</p><h3 id="PG状态的含义"><a href="#PG状态的含义" class="headerlink" title="PG状态的含义"></a>PG状态的含义</h3><p>PG的非正常状态说明可以参考[2]，<strong>undersized</strong>与<strong>degraded</strong>的含义记录于此：</p><blockquote><p>undersized<br>The placement group has fewer copies than the configured pool replication level.<br>degraded<br>Ceph has not replicated some objects in the placement group the correct number of times yet.<br>这两种状态一般同时出现，大概的意思就是有些PG没有满足设定的replicas数量要求，PG中的部分objects亦如此。看下PG的详细信息：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ceph health detail</span><br><span class="line">HEALTH_WARN 2 pgs degraded; 2 pgs stuck degraded; 2 pgs stuck unclean; 2 pgs </span><br><span class="line">stuck undersized; 2 pgs undersized</span><br><span class="line">pg 17.58 is stuck unclean <span class="keyword">for</span> 61033.947719, current state </span><br><span class="line">active+undersized+degraded, last acting [2,0]</span><br><span class="line">pg 17.16 is stuck unclean <span class="keyword">for</span> 61033.948201, current state </span><br><span class="line">active+undersized+degraded, last acting [0,2]</span><br><span class="line">pg 17.58 is stuck undersized <span class="keyword">for</span> 61033.343824, current state </span><br><span class="line">active+undersized+degraded, last acting [2,0]</span><br><span class="line">pg 17.16 is stuck undersized <span class="keyword">for</span> 61033.327566, current state </span><br><span class="line">active+undersized+degraded, last acting [0,2]</span><br><span class="line">pg 17.58 is stuck degraded <span class="keyword">for</span> 61033.343835, current state </span><br><span class="line">active+undersized+degraded, last acting [2,0]</span><br><span class="line">pg 17.16 is stuck degraded <span class="keyword">for</span> 61033.327576, current state </span><br><span class="line">active+undersized+degraded, last acting [0,2]</span><br><span class="line">pg 17.16 is active+undersized+degraded, acting [0,2]</span><br><span class="line">pg 17.58 is active+undersized+degraded, acting [2,0]</span><br></pre></td></tr></table></figure><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>虽然设定的拷贝数量是3，但是PG 17.58与17.58却只有两个拷贝，分别存放在OSD 0与OSD 2上。<br>而究其原因则是我们的OSD所在的磁盘不是同质的，从而每个OSD的weight不同，而Ceph对异质OSD的支持不是很好。从而导致部分PG无法满足我们设定的备份数量限制。</p><p>OSD状态树：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ceph osd tree</span><br><span class="line">ID WEIGHT  TYPE NAME      UP/DOWN REWEIGHT PRIMARY-AFFINITY</span><br><span class="line">-1 5.89049 root default</span><br><span class="line">-2 1.81360     host ceph3</span><br><span class="line">2 1.81360         osd.2       up  1.00000          1.00000</span><br><span class="line">-3 0.44969     host ceph4</span><br><span class="line">3 0.44969         osd.3       up  1.00000          1.00000</span><br><span class="line">-4 3.62720     host ceph1</span><br><span class="line">0 1.81360         osd.0       up  1.00000          1.00000</span><br><span class="line">1 1.81360         osd.1       up  1.00000          1.00000</span><br></pre></td></tr></table></figure><p>解决办法是另外构建一个OSD，使其容量大小和其它节点相同，是否可以有偏差？猜测应该有一个可以接受的偏差范围，重构后的OSD节点树看起来像这样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd tree</span><br><span class="line">ID WEIGHT  TYPE NAME      UP/DOWN REWEIGHT PRIMARY-AFFINITY</span><br><span class="line">-1 7.25439 root default</span><br><span class="line">-2 1.81360     host ceph3</span><br><span class="line"> 2 1.81360         osd.2       up  1.00000          1.00000</span><br><span class="line">-3       0     host ceph4</span><br><span class="line">-4 3.62720     host ceph1</span><br><span class="line"> 0 1.81360         osd.0       up  1.00000          1.00000</span><br><span class="line"> 1 1.81360         osd.1       up  1.00000          1.00000</span><br><span class="line">-5 1.81360     host ceph2</span><br><span class="line"> 3 1.81360         osd.3       up  1.00000          1.00000</span><br></pre></td></tr></table></figure><p>ceph4节点被删除，重新加入了另一个OSD节点ceph2。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ ceph -s</span><br><span class="line">    cluster 20ab1119-a072-4bdf-9402-9d0ce8c256f4</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e2: 2 mons at &#123;ceph2=192.168.17.21:6789/0,ceph4=192.168.17.23:6789/0&#125;</span><br><span class="line">            election epoch 26, quorum 0,1 ceph2,ceph4</span><br><span class="line">     osdmap e599: 4 osds: 4 up, 4 <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v155011: 100 pgs, 1 pools, 18628 bytes data, 1 objects</span><br><span class="line">            1129 MB used, 7427 GB / 7428 GB avail</span><br><span class="line">                 100 active+clean</span><br></pre></td></tr></table></figure><p>另外，为了满足HA的要求，OSD需要分散在不同的节点上，这里拷贝数量为3，则需要有三个OSD节点来承载这些OSD，如果三个OSD分布在两个OSD节点上，则依然可能会出现”active+undersized+degraded”的状态。</p><p>官方是这样说的：</p><blockquote><p>This, combined with the default CRUSH failure domain, ensures that replicas or erasure code shards are separated across hosts and a single host failure will not affect availability.</p></blockquote><p>理解如有错误还望能点醒。</p><hr><p>[1] <a href="https://www.mail-archive.com/ceph-users@lists.ceph.com/msg47070.html" target="_blank" rel="noopener">https://www.mail-archive.com/ceph-users@lists.ceph.com/msg47070.html</a><br>[2] <a href="http://docs.ceph.com/docs/master/rados/operations/pg-states/" target="_blank" rel="noopener">http://docs.ceph.com/docs/master/rados/operations/pg-states/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;自己搭的3个OSD节点的集群的健康状态经常处在”WARN”状态，replicas设置为3，OSD节点数量大于3，存放的data数量也不多，&lt;strong&gt;ceph -s&lt;/strong&gt; 不是期待的health ok，而是&lt;strong&gt;active+undersized+
      
    
    </summary>
    
    
      <category term="Ceph" scheme="http://jungler.com/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Dashboard</title>
    <link href="http://jungler.com/2018/06/18/%E5%88%9B%E5%BB%BAKubernetes-UI/"/>
    <id>http://jungler.com/2018/06/18/创建Kubernetes-UI/</id>
    <published>2018-06-18T04:32:53.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p>创建Kubernets Dashboard(UI)比较简单也就是几个命令的事儿，记录在这里作为一个备忘：</p><ul><li>通过官网提供的yaml配置文件创建service, role, deployment等等:</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</span><br><span class="line"></span><br><span class="line">secret <span class="string">"kubernetes-dashboard-certs"</span> created</span><br><span class="line">serviceaccount <span class="string">"kubernetes-dashboard"</span> created</span><br><span class="line">role <span class="string">"kubernetes-dashboard-minimal"</span> created</span><br><span class="line">rolebinding <span class="string">"kubernetes-dashboard-minimal"</span> created</span><br><span class="line">deployment <span class="string">"kubernetes-dashboard"</span> created</span><br><span class="line">service <span class="string">"kubernetes-dashboard"</span> created</span><br></pre></td></tr></table></figure><ul><li>启动kubectl proxy服务，API server将监听在8001端口，apiserver将负责访问控制：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl proxy --address=<span class="string">'0.0.0.0'</span> --port=8001 --accept-hosts=<span class="string">'.*'</span> &amp;</span><br></pre></td></tr></table></figure><p><strong>–address=’0.0.0.0’</strong> 使得其它机器也可以访问8001端口，<strong>–accept-hosts</strong> 让apiserver接受其它所有机器的请求。</p><ul><li>赋予Dashboard的serviceaccount以admin权限，亦或理解为直接绕过访问控制？因为我的环境是直接通过kubeadm安装的，所以可以创建一个文件名为dashboard-admin.yaml，文件内容如下：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure><p>执行下面的命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f dashboard-admin.yaml</span><br><span class="line">clusterrolebinding <span class="string">"kubernetes-dashboard"</span> created</span><br></pre></td></tr></table></figure><p>接下来就可以直接8001直接访问UI了，对于出现的用户认证页面，直接skip就好了，看到的界面看起来像这样：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/k8s-ui.png" alt=""></p><hr><p>[1] <a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/</a><br>[2] <a href="https://github.com/kubernetes/dashboard/wiki/Access-control" target="_blank" rel="noopener">https://github.com/kubernetes/dashboard/wiki/Access-control</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;创建Kubernets Dashboard(UI)比较简单也就是几个命令的事儿，记录在这里作为一个备忘：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过官网提供的yaml配置文件创建service, role, deployment等等:&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="http://jungler.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Python与协程</title>
    <link href="http://jungler.com/2018/05/26/Python%E4%B8%8E%E5%8D%8F%E7%A8%8B/"/>
    <id>http://jungler.com/2018/05/26/Python与协程/</id>
    <published>2018-05-26T09:36:10.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p>这篇博文对Python的协程(Coroutines)做个小结，记得多年之前在学校之时，学到的是进程，线程。即便工作之后用Python开始编写代码了，也很少会用到协程。那么协程是什么？提到协程我们得先去了解一下Python生成器(generator)与迭代器(iterator)。</p><p>简单来说迭代器(iterator)是一个你可以去顺序遍历的对象，是一个可以迭代(iterable)的Python类实例化的对象，这样的类一般来说是一个Python的容器类型，常见的容器类型如list，tuple，string都是可以迭代的(iterable)，也就是说通过它们可以实例化为迭代器，有点拗口，下面这张图可以帮助理解：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/relationships.png" alt="" title="我的博客"></p><p>迭代器必须实现<code>__iter__</code>与<code>__next__</code>两种内置方法，迭代器的优势在于节省内存的开销，例如下面的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">xrange</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, max)</span>:</span></span><br><span class="line">        self.i = <span class="number">0</span></span><br><span class="line">        self.max = max</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.i &lt; self.max:</span><br><span class="line">            i = self.i</span><br><span class="line">            self.i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> StopIteration()</span><br></pre></td></tr></table></figure><p>每次调用<code>next()</code>会生成一个元素，这有点类似<code>C</code>语言里的指针，这对于无需一次访问整个列表而只访问某个元素来说是非常有益的，因为<em>内存</em>的占用始终是一个常数。如果返回的是一个列表类型的话，将会将所有的结果都保存在内存中，如果max特别大的话，消耗的内存将是一个必须考虑的问题。</p><p>生成器是一个特殊的迭代器，通过引入<code>yield</code>关键字简化了迭代器的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xrange</span><span class="params">(max)</span>:</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; max:</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line">        i += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>与上面的代码作用相同，每调用<code>next()</code>方法生成一个值直到值等于<code>max</code>为止。在某些场合下或者某些语言中，一个生成器可以理解为一个协程，但是又有细微的不同:</p><ul><li>生成器一般用来产生数据</li><li>协程一般用来消费数据</li></ul><p>但这是通常的使用场合的不同，实现上看不出来什么区别，也就是说如果我们需要对<code>yield</code>出来的数据做进一步的处理，则可以将其理解为协程。</p><p>协程有时和多线程可以用来实现相同的目的，那协程相对于线程来说有什么不同和优势呢？最明显的区别在于协程将控制权交由程序自己来控制，本质上就是一个线程，因为没有多线程场景下的CPU中断，也没有线程切换，自然系统的开销就要小的多，那么多核的平台上，又要并发，又要灵活的控制程序的执行，则可以将协程和多线程结合起来。下面来看一个协程的例子:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">grep</span><span class="params">(pattern)</span>:</span></span><br><span class="line">    print(<span class="string">"Searching for"</span>, pattern)</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        line = (<span class="keyword">yield</span>)  <span class="comment"># 2</span></span><br><span class="line">        <span class="keyword">if</span> pattern <span class="keyword">in</span> line:</span><br><span class="line">            print(line)</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>search = grep(<span class="string">"hello"</span>)  <span class="comment"># 0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(search)         <span class="comment"># 1</span></span><br><span class="line">(<span class="string">'Searching for'</span>, <span class="string">'hello'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>search.send(<span class="string">"i love you"</span>) <span class="comment"># 3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>search.send(<span class="string">"helloworld"</span>)  <span class="comment"># 4</span></span><br><span class="line">helloworld</span><br></pre></td></tr></table></figure><p>0: 初始化此协程<br>1: 用<code>next()</code>方法来启动此协程，line此时被赋值为”hello”，且在“2”处暂停。<br>3：调用<code>send()</code>方法来匹配字符串”hello”，因为没有匹配上所以没有输出。<br>4：继续匹配“hello”，匹配成功，输出匹配结果。程序继续在2处暂停。</p><p><code>Python</code>语言中，对协程支持的并不是很好，目前看来，<code>greenlet</code>对<code>yield</code>进行封装实现对协程的支持，<code>Eventlet</code>再对<code>greenlet</code>进行了二次封装，后面有空再对<code>greenlet</code>和<code>Eventlet</code>做个小结吧。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇博文对Python的协程(Coroutines)做个小结，记得多年之前在学校之时，学到的是进程，线程。即便工作之后用Python开始编写代码了，也很少会用到协程。那么协程是什么？提到协程我们得先去了解一下Python生成器(generator)与迭代器(iterator
      
    
    </summary>
    
    
      <category term="Python" scheme="http://jungler.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Patch 学习小结</title>
    <link href="http://jungler.com/2018/05/20/kube-patch/"/>
    <id>http://jungler.com/2018/05/20/kube-patch/</id>
    <published>2018-05-20T02:00:12.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p>总结一下<code>Kubernetes Patch</code>的使用方法，详细的说明还要要去看官网[1]，K8S用这个命令来对运行中的应用进行动态跟新。总的来说<code>Patch</code>一个API对象有三种方式：</p><ul><li>使用JSON Patch来更新一个对象，没有看到具体的例子，看起来这是JSON的一个标准[2]，类似数据库的增删改查的方式对原来的JSON格式进行修改，不太清楚K8S对其支持如何。</li><li>使用JSON merge patch，这种方式需要定义一个完整的替换列表，也就是说，新的列表定义会替换原有的定义。</li><li>使用JSON strategic merge patch，这种补丁是以增量的形式来对已有的定义进行修改，可以理解为类似于<code>linux diff</code>创建的补丁。</li></ul><p>下面对第二种和第三种形式的更新，分别来举个栗子：</p><h4 id="JSON-merge-patch"><a href="#JSON-merge-patch" class="headerlink" title="JSON merge patch"></a>JSON merge patch</h4><p>下面的例子用来部署一个<code>nginx</code>应用，2份拷贝，后面在此基础上打补丁</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span> <span class="comment"># for versions before 1.9.0 use apps/v1beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">patch-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">patch-demo-ctr</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">        key:</span> <span class="string">dedicated</span></span><br><span class="line"><span class="attr">        value:</span> <span class="string">test-team</span></span><br></pre></td></tr></table></figure><p>先部署这个应用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f patch_demo.yaml</span><br><span class="line">$ kubectl get pod -o wide</span><br><span class="line"></span><br><span class="line">NAME                         READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">patch-demo-576f89c99-mf4fj   1/1       Running   0          1m        10.244.2.118   k8s-node2</span><br><span class="line">patch-demo-576f89c99-tb97t   1/1       Running   0          1m        10.244.1.185   k8s-node1</span><br></pre></td></tr></table></figure><p>可以两个pod都已经跑了起来，接下来想对image进行修改，定义下面的patch:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">patch-demo-ctr-3</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">gcr.io/google-samples/node-hello:1.0</span></span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl patch deployment patch-demo --<span class="built_in">type</span> merge --patch <span class="string">"<span class="variable">$(cat patch_image.yaml)</span>"</span></span><br><span class="line">$ kubectl get pod -o wide</span><br><span class="line">patch-demo-86c8577c88-bgd9s   1/1       Running   0          9m        10.244.2.119   k8s-node2</span><br><span class="line">patch-demo-86c8577c88-qqfrv   1/1       Running   0          10m       10.244.1.187   k8s-node1</span><br></pre></td></tr></table></figure><p>对比pod ID可见已有的pod已经被terminate了，并重新创建了两个新的 pod，可以进一步查看更新后的image：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get deployment patch-demo --output yaml | grep image</span><br><span class="line">      - image: gcr.io/google-samples/node-hello:1.0</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br></pre></td></tr></table></figure><h4 id="JSON-strategic-merge-patch"><a href="#JSON-strategic-merge-patch" class="headerlink" title="JSON strategic merge patch"></a>JSON strategic merge patch</h4><p>所谓策略性补丁，就是作为一个对已有配置的增量补丁，想想<code>diff</code>就好了，patch中没有定义的修改内容，则不会对原有配置产生影响，还是看下这个例子，基于原始版本新增一个新的<code>redis</code>的容器，定义好下面的补丁：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">patch-demo-ctr-2</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">redis</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl patch deployment patch-demo --patch <span class="string">"<span class="variable">$(cat patch_container.yaml)</span>"</span></span><br><span class="line">deployment <span class="string">"patch-demo"</span> patched</span><br><span class="line"></span><br><span class="line">$ kubectl get pod -o wide</span><br><span class="line">NAME                          READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">patch-demo-74b9844b77-hk2l7   2/2       Running   0          3m        10.244.2.120   k8s-node2</span><br><span class="line">patch-demo-74b9844b77-qjz6r   2/2       Running   0          5m        10.244.1.188   k8s-node1</span><br></pre></td></tr></table></figure><p><code>2/2</code>表示每个pod有两个容器，如果想再看细点，用下面命令来查看pod上跑的image:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod patch-demo-74b9844b77-hk2l7 --output yaml | grep image</span><br><span class="line">  - * image: redis *</span><br><span class="line">    imagePullPolicy: Always</span><br><span class="line">  - * image: gcr.io/google-samples/node-hello:1.0 *</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    image: redis:latest</span><br><span class="line">    imageID: docker-pullable://redis@sha256:4aed8ea5a5fc4cf05c8d5341b4ae4a4f7c0f9301082a74f6f9a5f321140e0cd3</span><br><span class="line">    image: gcr.io/google-samples/node-hello:1.0</span><br><span class="line">    imageID: docker-pullable://gcr.io/google-samples/node-hello@sha256:d238d0ab54efb76ec0f7b1da666cefa9b40be59ef34346a761b8adc2dd45459b</span><br></pre></td></tr></table></figure><p>好了，先总结到这里，详细的介绍还是看官网吧，下周再来看看。</p><hr><p>[1] <a href="https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/</a><br>[2] <a href="http://erosb.github.io/post/json-patch-vs-merge-patch/" target="_blank" rel="noopener">http://erosb.github.io/post/json-patch-vs-merge-patch/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;总结一下&lt;code&gt;Kubernetes Patch&lt;/code&gt;的使用方法，详细的说明还要要去看官网[1]，K8S用这个命令来对运行中的应用进行动态跟新。总的来说&lt;code&gt;Patch&lt;/code&gt;一个API对象有三种方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用JSON Pat
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="http://jungler.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>OpenStack 与 SDN --- namespace</title>
    <link href="http://jungler.com/2018/04/30/OpenStack-SDN-namespace/"/>
    <id>http://jungler.com/2018/04/30/OpenStack-SDN-namespace/</id>
    <published>2018-04-30T08:20:33.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<h3 id="什么是network-namespace"><a href="#什么是network-namespace" class="headerlink" title="什么是network namespace"></a>什么是network namespace</h3><p>先来看看linux手册上对namespace的解释</p><blockquote><p>A network namespace is logically another copy of the network stack, with its own routes, firewall rules, and network devices.<br>By default a process inherits its network namespace from its parent. Initially all the processes share the same default network namespace from the init process.</p></blockquote><p>翻译过来再加以理解，大致的意思就是network namespace实现了对网络资源的隔离，一个namespace有自己的路由，防火墙规则，以及网络设备。进程会从他的父进程那里继承network namespace。所有进程从共同的父进程init进程那里共享默认network namespace.</p><p>默认的network namespace有时也被称作root namespace，一个基本的原则是一个网络设备最终只能属于一个namespace，无论是物理设备或者虚拟设备。从实现上network namespace介于<strong>chroot</strong>与虚拟机VM之间，VM太重，而chroot不能有效实现网络设备隔离，如果想要实现的网络设备的隔离，那么优先考虑namespace。</p><p>从一张图来看或许更加清楚一些：<br><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/namespace.png" alt=""></p><h3 id="namespace与OpenStack"><a href="#namespace与OpenStack" class="headerlink" title="namespace与OpenStack"></a>namespace与OpenStack</h3><p>Neutron项目直接依赖于namespace，具体哪个版本引入不是太清楚了，如果你的环境是用<em>devstack</em>搭建起来的话，那么默认你是可以看到两个namespace的，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ip netns</span><br><span class="line">qrouter-a781c60f-3929-4a2b-b233-d724f3693d4e</span><br><span class="line">qdhcp-8c21785b-fdcf-49c6-8cf2-9ea56b9e8d35</span><br></pre></td></tr></table></figure><p>可以看到一个提供路由功能，另一个提供DHCP服务，创建多个子网并设置DHCP服务可以在系统上创建对个DHCP namespace。<br>进入到namesapce内部来一探究竟。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip netns <span class="built_in">exec</span> qrouter-a781c60f-3929-4a2b-b233-d724f3693d4e ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">42: qr-2e14aa33-ac: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:88:7b:28 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.0.0.1/26 brd 10.0.0.63 scope global qr-2e14aa33-ac</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::f816:3eff:fe88:7b28/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">43: qg-9ad90b84-19: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:33:37:8e brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.42.139/25 brd 192.168.42.255 scope global qg-9ad90b84-19</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.42.131/32 brd 192.168.42.131 scope global qg-9ad90b84-19</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 2001:db8::b/64 scope global</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::f816:3eff:fe33:378e/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">44: qr-9166b961-68: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:11:5f:29 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet6 fdbf:ae1d:a3f6::1/64 scope global</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::f816:3eff:fe11:5f29/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>对于router namespace来说<em>qr-2e14aa33-ac</em>其实就是一个内网的网关，<em>10.0.0.1/26</em>网段为内网网段，<em>qg-9ad90b84-19</em>则对应的是外网，其中<em>192.168.42.139</em>为外网网关，而其他IP地址则对应一个个用于访问外网的floating IP地址，他们共享一个虚拟的以太网地址或MAC地址。</p><p>dhcp namespace类似，其中除了一个回环设备外，剩下的就是一个IP地址就是DHCP服务器的IP地址。那么虚拟机是如何与他们产生联系的呢？答案是linux bridge或者OVS, 以OVS为例，这些虚拟的设备都会附加到<em>br-int</em> bridge上，对每一个虚拟机而言，又通过veth pair的方式附加到同一个bridge上以实现互联互通。不展开讨论。</p><p>OpenStack引入namespace的目的是为了解决多租户情况下的三层网络IP地址隔离，具体分析不用namespace的情况下可能会出现的问题可以参考这篇博文[1]</p><h3 id="namespace-访问外网"><a href="#namespace-访问外网" class="headerlink" title="namespace 访问外网"></a>namespace 访问外网</h3><p>谈到namespace对外网的访问，这里涉及到两个方面：</p><ol><li>namespace内部网络设备比方说之前提到的 <em>qg-9ad90b84-19</em>设备对外网的访问。</li><li>租户网络与外部网络之间的互访问。</li></ol><p>第一个问题的答案之前已经提及，主要是通过将网络设备绑定到linux bridge或者OVS bridge来实现。<br>第二个问题的答案是floating IP与iptable，floating IP的本质只是通过iptable实现的一些虚拟IP而非物理的绑定到网卡上的IP地址，那么通过查看router上的iptable定义的规则，我们可以很清楚的了解到floating IP的实现原理。</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/namespace-iptables.png" alt=""></p><p>上图中DNAT用于从外部网络访问虚拟机内部网络所定义的地址转换规则；SNAT用于从虚拟机内部网络访问外网是所需要的地址转换规则。</p><p>留下一个问题，namespace与LXC以及container这三者之间的关系是什么？留待下次总结。</p><hr><p>[1] <a href="https://blog.csdn.net/cloudman6/article/details/52876889" target="_blank" rel="noopener">https://blog.csdn.net/cloudman6/article/details/52876889</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;什么是network-namespace&quot;&gt;&lt;a href=&quot;#什么是network-namespace&quot; class=&quot;headerlink&quot; title=&quot;什么是network namespace&quot;&gt;&lt;/a&gt;什么是network namespace&lt;/h3&gt;&lt;
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>我的数据在哪儿? - Ceph rbd image</title>
    <link href="http://jungler.com/2018/04/21/where-is-my-data/"/>
    <id>http://jungler.com/2018/04/21/where-is-my-data/</id>
    <published>2018-04-21T05:06:14.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/sunshine.jpg" alt=""></p><p>Ceph的rbd image可以用来作为OpenStack的块存储，如果OpenStack配置Cinder存储后端为Ceph，实际上读写的就是Ceph的块存储设备，这里记录如何查看rbd image里的数据，以及数据存放在哪里。</p><p>首先来创建一个rbd image</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create rbdbench 100 100  <span class="comment">#创建一个名为rbdbench的pool，pg与pgp size均为100</span></span><br><span class="line"></span><br><span class="line">rbd create image01 --size 1024 --pool rbdbench --image-format 2  <span class="comment"># format 1已经deprecated了，format 2 包含了更多的特性。</span></span><br></pre></td></tr></table></figure><p>这里不需要做rbd的mapping操作，也无需mount rbd image，我们只想来看看rbd image里文件存放位置，如果需要做mapping，则需要修改ceph的主配置文件来忽略系统不支持的一些ceph的特性。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'$a\rbd_default_features = 3'</span> /etc/ceph/ceph.conf</span><br></pre></td></tr></table></figure><p>接下来，我们通过rbd info查看image的一些详细信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ rbd -p rbdbench info image01</span><br><span class="line">rbd image <span class="string">'image01'</span>:</span><br><span class="line">        size 1024 MB <span class="keyword">in</span> 256 objects</span><br><span class="line">        order 22 (4096 kB objects)</span><br><span class="line">        block_name_prefix: rbd_data.4dde74b0dc51</span><br><span class="line">        format: 2</span><br><span class="line">        features: layering</span><br><span class="line">        flags:</span><br></pre></td></tr></table></figure></p><p>块设备里已经有256个object了，这些个object是什么，我们以后再看，通过block_name_prefix来查看pool里的objects.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ rados -p rbdbench ls | grep ^rbd_data.4dde74b0dc51</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000060</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000086</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000084</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000081</span><br><span class="line">rbd_data.4dde74b0dc51.00000000000000e0</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000083</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000000</span><br><span class="line">rbd_data.4dde74b0dc51.00000000000000a0</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000080</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000004</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000082</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000085</span><br><span class="line">rbd_data.4dde74b0dc51.00000000000000ff</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000087</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000020</span><br></pre></td></tr></table></figure></p><p>接下来就可以通过下面的命令来查找object所在的pg以及相应的OSD了。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd map rbdbench rbd_data.4dde74b0dc51.0000000000000086</span><br><span class="line">osdmap e505 pool <span class="string">'rbdbench'</span> (16) object <span class="string">'rbd_data.4dde74b0dc51.0000000000000086'</span> -&gt; pg 16.eabd8f8a (16.a) -&gt; up ([2,1], p2) acting ([2,1], p2)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd tree</span><br><span class="line">ID WEIGHT  TYPE NAME      UP/DOWN REWEIGHT PRIMARY-AFFINITY</span><br><span class="line">-1 5.44080 root default</span><br><span class="line">-2 1.81360     host ceph3</span><br><span class="line"> 2 1.81360         osd.2       up  1.00000          1.00000</span><br><span class="line">-3       0     host ceph4</span><br><span class="line">-4 3.62720     host ceph1</span><br><span class="line"> 0 1.81360         osd.0       up  1.00000          1.00000</span><br><span class="line"> 1 1.81360         osd.1       up  1.00000          1.00000</span><br></pre></td></tr></table></figure><p>数据所在的主OSD为2， 从OSD为1， pg号为“16.a”，这样登录到OSD所在的机器，就可以查看到object的data文件了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ceph3:/var/lib/ceph/osd/ceph-2/current/16.a_head<span class="comment"># file rbd\\udata.4dde74b0dc51.0000000000000086__head_EABD8F8A__10</span></span><br><span class="line">rbd\udata.4dde74b0dc51.0000000000000086__head_EABD8F8A__10: data</span><br></pre></td></tr></table></figure><p>有几个问题，rbd初始创建的object到底是什么？除了“block_name_prefix”指定的object之外，还有哪些objects? 可否通过这种方式创建一个image，然后写入一个文件，再去查看文件的存储位置，以及完整性校验等。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://github.com/chendave/chendave.github.io/raw/master/css/images/sunshine.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Ceph的rbd image可以用来作为OpenStack
      
    
    </summary>
    
    
      <category term="Ceph" scheme="http://jungler.com/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>OpenStack Queen 版本变更概述</title>
    <link href="http://jungler.com/2018/04/14/openstack-queen/"/>
    <id>http://jungler.com/2018/04/14/openstack-queen/</id>
    <published>2018-04-14T07:44:53.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/chendave/initrepo/master/pic/banner.jpg" alt=""></p><p>毫无疑问，OpenStack正在经历它的低谷期，和芸芸众生一样，无法改变世界那就得改变自己来适应这个世界，真心的期待，曾经的王者有朝一日能再现昔日辉煌。</p><p>回过头来看Queen版本的一些主要变更，Mirantis的这篇文章[1] 总结的不错，</p><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>Queen版本之前，使用GPU做科学计算和机器学习，可以通过使用PCI pass through或者直接用Ironic来操作裸机，Queen版本增加了新的flavor，可以像其它flavor例如vCPUs一样来支持对GPU资源作出的请求。</p><h3 id="高可用性"><a href="#高可用性" class="headerlink" title="高可用性"></a>高可用性</h3><p>个人觉得这个特性对OpenStack的落地是个很大的提高，尤其是虚拟机层次的高可用性， 围绕这个特性，块存储以及裸机管理方面的相关功能也都值得期待。</p><h3 id="边缘计算"><a href="#边缘计算" class="headerlink" title="边缘计算"></a>边缘计算</h3><p>围绕边缘计算和容器化，我们可以看到新的项目如LOCI和OpenStack-Helm所作出的努力，OpenStack容器化已经有些年头，而一时间出来这么多和Container相关的项目也算是OpenStack受诟病的一个原因吧，能否多这么的项目做个整合，再比如早起的Kolla?</p><p>下个版本据说是要更加关注NFV了，祝OpenStack一路走好吧！</p><p>附原文：</p><p>OpenStack embraces the future with GPU, edge computing support</p><p>It wasn’t that long ago that OpenStack was the hot new kid on the infrastructure block. Lately, though, other technologies have been vying for that spot, making the open source cloud platform look downright stodgy in comparison. That just might change with the latest release of OpenStack, code-named Queens.</p><p>The Queens release makes it abundantly clear that the OpenStack community, far from resting on its laurels or burying its collective head in the digital sand, has been paying attention to what’s going on in the cloud space and adjusting its efforts accordingly. Queens includes capabilities that wouldn’t even have been possible when the OpenStack project started, let alone considered, such as GPU support (handy for scientific and machine learning/AI workloads) and a focus on Edge Computing that makes use of the current new kid on the block, Kubernetes.</p><p><strong>Optimization</strong></p><p>While OpenStack users have been able to utilize GPUs for scientific and machine learning purposes for some time, it has typically been through the use of either PCI passthrough or by using Ironic to manage an entire server as a single instance — neither of which was particularly convenient. Queens now makes it possible to provision virtual GPUs (vGPUs) using specific flavors, just as you would provision traditional vCPUs.</p><p>Queens also includes the debut of the Cyborg project, which provides a management framework for different types of accelerators such as GPUs, FPGA, NVMe/NOF, SSDs, DPDK, and so on. This capability is important not just for GPU-related use cases, but also for situations such as NFV.</p><p><strong>High Availability</strong></p><p>As OpenStack becomes more of an essential tool and less of a science project, the need for high availability has grown. The OpenStack Queens release addresses this need in several different ways.</p><p>The OpenStack Instances High Availability Service, or Masakari, provides an API to manage the automated rescue mechanism that recovers instances that fail because of process down, provisioning process down, or nova-compute host failure events.</p><p>While Masakari currently supports KVM-based VMs, Ironic bare metal nodes have always been more difficult to recover. Queens debuts the Ironic Rescue Mode (one of our favorite feature names of all time), which makes it possible to recover an Ironic node that has gone down.</p><p>Another way OpenStack Queens provides HA capabilities is through Cinder’s new volume multi-attach feature. The OpenStack Block Storage Service’s new capability makes it possible to attach a single volume to multiple VMs, so if one of those instances fails, traffic can be routed to an identical instance that is using the same storage.</p><p><strong>Edge Computing</strong></p><p>What’s become more than obvious, though, is that OpenStack has realized that the future doesn’t lay in just a few concentrated datacenters, but rather that workloads will be in a variety of diverse locations. Specifically, Edge Computing, in which we will see multiple smaller clouds closer to the user rather than a single centralized cloud, is coming into its own as service providers and others realize its importance.</p><p>To that end, OpenStack has been focused on several projects to adapt itself to that kind of environment, including LOCI and OpenStack-Helm.</p><p>OpenStack LOCI provides Lightweight OCI compatible images of OpenStack services so that they can be deployed by a container orchestration tool such as Kubernetes. As of the Queens release, images are available for Cinder, Glance, Heat, Horizon, Ironic, Keystone, Neutron and Nova.</p><p>And of course since orchestrating a containerized deployment of OpenStack isn’t necessarily any easier than deploying a non-containerized version, there’s OpenStack-Helm, a collection of Helm charts that install the various OpenStack services on a Kubernetes cluster.</p><p><strong>Other container-related advances</strong></p><p>If it seems like there’s a focus on integrating with container-based services, you’re right. Another way OpenStack has integrated with Kubernetes is through the Kuryr CNI plugin. The Container Network Interface (CNI) is a CNCF project that standardizes container networking operations, and the Kuryr CNI plugin makes it possible to use OpenStack Neutron within your Kubernetes cluster.</p><p>Also, if your container needs are more modest — maybe you don’t need an actual cluster, you just want the containers — the new Zun project makes it possible to run application containers on their own.</p><p><strong>Coming up next</strong></p><p>As always, it’s impossible to sum up 6 months of OpenStack work in a single blog post, but the general idea is that the OpenStack community is clearly thinking about the long term future and planning accordingly. While this release focused on making it possible to run OpenStack at the Edge, the next, code-named Rocky, will see a focus on NFV-related functionality such as minimum bandwidth requirements to ensure service quality.</p><p>What’s more, the community is also working on “mutable configuration across services”, which means that as we move into Intelligent Continuous Delivery (ICD) and potentially ever-changing and morphing infrastructure, we’ll be able to change service configurations without having to restart services.</p><hr><p>[1] <a href="https://www.mirantis.com/blog/openstack-embraces-the-future-with-gpu-edge-computing-support/" target="_blank" rel="noopener">https://www.mirantis.com/blog/openstack-embraces-the-future-with-gpu-edge-computing-support/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chendave/initrepo/master/pic/banner.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;毫无疑问，OpenStack正在经历它的低谷期，和芸芸众生一样，无法改变世
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 环境搭建与 proxy设置</title>
    <link href="http://jungler.com/2018/04/06/Kubernetes-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E4%B8%8E-proxy%E8%AE%BE%E7%BD%AE/"/>
    <id>http://jungler.com/2018/04/06/Kubernetes-环境搭建与-proxy设置/</id>
    <published>2018-04-06T14:53:38.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/鲁迅公园.jpg" alt=""><br>从18年开始，计划投入大力气学习Kubernetes，毕竟还是想走云计算这条路，当技术变革来临之时，与其被拍在沙滩上还不如伸开双手去拥抱。</p><p>既然下定决心去学习Kubernetes，那就得脚踏实地的去做点事情，搭建一个实验的Kubernetes 集群算作是第一步吧。</p><h2 id="为什么要设置代理"><a href="#为什么要设置代理" class="headerlink" title="为什么要设置代理"></a>为什么要设置代理</h2><p>接触到第一个部署Kubernetes的工具是kubeadm，用Kubernetes 搭建一个集群官网[1]有比较详细的描述，步骤也比较简单，这里不打算重复kubeadm的几个命令，而是着重吐槽一下有关代理设置这一问题，在我们伟大的祖国，想做点事，代理就越发凸显出它的重要性，谁让这些开源技术都是人家美帝弄出来的？言归正传，搭建Kubernetes 必须要用到proxy，这是因为很多docker的image都是由Google在维护，即便不是，为了image的下载速度还可以接受，我们也得有个梯子。</p><h3 id="设置终端代理"><a href="#设置终端代理" class="headerlink" title="设置终端代理"></a>设置终端代理</h3><p>在执行<strong>kubeadm init</strong>去初始化master节点之前，用下面的命令去设置终端代理：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> http_proxy=http://<span class="variable">$username</span>:<span class="variable">$password</span>@<span class="variable">$proxy_host</span>:<span class="variable">$port</span></span><br><span class="line">$ <span class="built_in">export</span> https_proxy=https://<span class="variable">$username</span>:<span class="variable">$password</span>@<span class="variable">$proxy_host</span>:<span class="variable">$port</span></span><br><span class="line">$ <span class="built_in">export</span> no_proxy=127.0.0.1,localhost,192.168.2.100</span><br></pre></td></tr></table></figure></p><p>192.168.2.100是master节点的物理IP。</p><h3 id="设置docker的代理"><a href="#设置docker的代理" class="headerlink" title="设置docker的代理"></a>设置docker的代理</h3><p>初始化的过程中是要通过docker去下载image，没有代理去下载？你就去等吧，等吧，终于等到timeout。</p><p>可以去docker的官网去看如何为docker设置代理，这里记录我在实验环境里的设置，或许还需要用相同的方式创建一个https的proxy文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p /etc/systemd/system/docker.service.d</span><br><span class="line"></span><br><span class="line">$ cat /etc/systemd/system/docker.service.d/http-proxy.conf</span><br><span class="line"></span><br><span class="line">[Service]     </span><br><span class="line">Environment=<span class="string">"HTTP_PROXY=http://<span class="variable">$username</span>:<span class="variable">$password</span>@<span class="variable">$proxy_host</span>:<span class="variable">$port</span>"</span> <span class="string">"NO_PROXY=localhost,127.0.0.1,192.168.2.100"</span></span><br></pre></td></tr></table></figure><h3 id="初始化master节点"><a href="#初始化master节点" class="headerlink" title="初始化master节点"></a>初始化master节点</h3><p>既然代理都已经设置好了（其实这里有一个坑，很大的一个坑 ^^），来点真格的吧。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubeadm init --apiserver-advertise-address 192.168.2.100 --pod-network-cidr=10.244.0.0/16</span></span><br></pre></td></tr></table></figure><p>给出来的是下面一堆输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[init] Using Kubernetes version: v1.9.3</span><br><span class="line">[init] Using Authorization modes: [Node RBAC]</span><br><span class="line">[preflight] Running pre-flight checks.</span><br><span class="line">        [WARNING FileExisting-crictl]: crictl not found <span class="keyword">in</span> system path</span><br><span class="line">        [WARNING HTTPProxyCIDR]: connection to <span class="string">"10.96.0.0/12"</span> uses proxy <span class="string">"http://\$username:\$password@\$proxy_host:<span class="variable">$port</span>"</span>. This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions <span class="keyword">in</span> proxy configuration</span><br><span class="line">...</span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">You can now join any number of machines by running the following on each node</span><br><span class="line">as root:</span><br><span class="line">  kubeadm join --token 5076b0.10c90eec17e4a2a3 192.168.2.100:6443 --discovery-token-ca-cert-hash sha256:b7cdd4209d7357a020d29ca92f9b99ce1b671cd2fe841ca24ff7114e50f8778f</span><br></pre></td></tr></table></figure></p><p>嗯，虽然有两个warning，但还是命令执行成功了。十年来的IT“从业经验”告诉我，警告看的多了，你吓唬的了谁？你看不是最终还是成功了么？</p><p>接着添加其它两个node节点到K8S的cluster里，还是成功！</p><p>耍一把K8S的命令行工具，依然是成功！</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes</span><br><span class="line">NAME        STATUS    ROLES     AGE       VERSION</span><br><span class="line">k8s-node1   Ready     &lt;none&gt;    10d       v1.9.6</span><br><span class="line">k8s-node2   Ready     &lt;none&gt;    8d        v1.9.3</span><br><span class="line">k8smaster   Ready     master    10d       v1.9.3</span><br></pre></td></tr></table></figure><p>直到有一天我要去查看pod的日志文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl logs -f httpd-7448fc6b46-6pf7w</span><br><span class="line">Error from server: Get https://192.168.18.111:10250/containerLogs/default/httpd-7448fc6b46-6pf7w/httpd?follow=<span class="literal">true</span>: cannotconnect</span><br></pre></td></tr></table></figure></p><p>然而访问master节点上的日志却正常，pod也运行正常，通过curl也可以正常访问。</p><p>尝试K8S的dashboard，虽然可以安装成功，但页面访问给出的也是一样的提示<strong>cannotconnect</strong>，百思不得其解啊，你到是为什么<strong>cannotconnect</strong>？！！ sun of the beach !!</p><p>直觉告诉我，这又是一个proxy设置的问题，但无论如何也想不到错在哪里，终于有那么一天，当我老老实实的复盘时，我不得不再次审视那几个warning。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[preflight] Running pre-flight checks.</span><br><span class="line">        [WARNING FileExisting-crictl]: crictl not found <span class="keyword">in</span> system path</span><br><span class="line">        [WARNING HTTPProxyCIDR]: connection to <span class="string">"10.96.0.0/12"</span> uses proxy <span class="string">"http://\$username:\$password@\$proxy_host:<span class="variable">$port</span>"</span>. This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions <span class="keyword">in</span> proxy configuration</span><br></pre></td></tr></table></figure></p><p>恍然大悟，<strong>no_proxy</strong>, 不光是这里的”10.96.0.0/12”（虽然到现在我也不知道这个网络是给谁用的，-_-||），给docker设置proxy时，各个slave节点的IP也得在no_proxy的设置范围内，回过头来看，我们的proxy设置应该想像下面这样，其中192.168.18.111,192.168.18.75是slave节点的物理IP地址。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 终端proxy</span></span><br><span class="line">$ <span class="built_in">export</span> no_proxy=127.0.0.1,localhost,192.168.2.100，192.168.18.111,10.96.0.0/12,192.168.18.75,10.244.0.0/16</span><br><span class="line"><span class="comment"># docker 代理</span></span><br><span class="line">$ cat /etc/systemd/system/docker.service.d/http-proxy.conf</span><br><span class="line"></span><br><span class="line">[Service]     </span><br><span class="line">Environment=<span class="string">"HTTP_PROXY=http://<span class="variable">$username</span>:<span class="variable">$password</span>@<span class="variable">$proxy_host</span>:<span class="variable">$port</span>"</span> <span class="string">"NO_PROXY=localhost,127.0.0.1,192.168.2.100，192.168.18.111,192.168.18.75,10.244.0.0/16"</span></span><br></pre></td></tr></table></figure><p>通过下面的命令，让配置的代理生效：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure><p>确认更新后的代理设置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl show docker --property Environment</span><br></pre></td></tr></table></figure><p>再去试试CURL，dashboard，kubectl logs一切如你所愿！</p><hr><p>[1] <a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm" target="_blank" rel="noopener">https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://github.com/chendave/chendave.github.io/raw/master/css/images/鲁迅公园.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;从18年开始，计划投入大力气学习Kubernetes，毕竟还是想走云计算这
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="http://jungler.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://jungler.com/2018/03/06/hello-world/"/>
    <id>http://jungler.com/2018/03/06/hello-world/</id>
    <published>2018-03-06T07:44:32.000Z</published>
    <updated>2020-02-08T11:07:23.964Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
